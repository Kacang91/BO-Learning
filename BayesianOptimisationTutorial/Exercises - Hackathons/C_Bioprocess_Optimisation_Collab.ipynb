{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6023a9d2-d1b5-4383-9701-7131b0a6ed0b",
      "metadata": {
        "id": "6023a9d2-d1b5-4383-9701-7131b0a6ed0b"
      },
      "source": [
        "### Optimisation of a Bioprocess with Multifidelity Bayesian Optimisation\n",
        "\n",
        "\n",
        "#### Hackathon Breif\n",
        "This hackathon involves the optimisation of a simulated bioprocess at process scale utilising CHO cells to produce a desired protein. Experimentally, this would involve a resource-intensive screening campaign involving the growth and feeding of cells under precise conditions (temperature, pH, feed amount, fidelity, etc.) to maximize the production of a desired product. This hackathon offers a simulated method of mapping bioprocess input parameters to a final predicted titre concentration: a measure of cell productivity. The simulations are based on various kinetic parameters which are unique to the type of cells used. For the final scoring, a different set of cell kinetic parameters will be used to evaluate your algorithm.\n",
        "\n",
        "#### Inputs and Outputs\n",
        "Inputs to the bioprocess includes 5 vairables: the temperature [°C], pH and the concentration of feed [mM] at 3 different timepoints over 150 minutes. The output is the concentration of the titre (desired product) [g/L]. The goal is to obtain the input variables that correspond to the highest obtained titre.\n",
        "\n",
        "The bounds of the inputs are as follows:\n",
        "\n",
        "```\n",
        "temperature [°C]               -> 30 - 40\n",
        "pH                             -> 6 - 8\n",
        "first feed concentration [mM]  -> 0 - 50\n",
        "second feed concentration [mM] -> 0 - 50\n",
        "third feed concentration [mM]  -> 0 - 50\n",
        "```\n",
        "\n",
        "#### Fidelities and Running the simulation\n",
        "The simulations can be perfomed at 3 levels of fidelities with an associated accuracy and costs. These fidelities corresponds to a different reactor type and scale used.\n",
        "\n",
        "```\n",
        "Lowest fideility: 3L reactor with 1 feeding timepoint at 60 mins.\n",
        "Realtive cost: 10\n",
        "Remarks: The feeding concentration is taken as the second feed concentration. Lowest accuracy, but also lowest cost.\n",
        "\n",
        "Middle fidelity: 3L reactor with 3 feeding timepoints at 40, 80, 120 mins.\n",
        "Relative cost: 575\n",
        "Remarks: -\n",
        "\n",
        "Highest fidelity: 15L reactor with 3 feeding timepoints at 40, 80, 120 mins.\n",
        "Relative cost: 2100\n",
        "Remarks: Highest accuracy but high cost.\n",
        "```\n",
        "\n",
        "To run an experiment, one can use the `conduct_experiment(X)` function -> this is your objective function. The inputs to this function is a matrix of shape (N, 6) where N is the number of data points and 6 refers to the total number of variables in the following order: `[temperature, pH, feed1, feed2, feed3, fidelity]`. The fidelities are refered to as integers where `0` corresponds to the lowest fidelity, `1` with the middle and `2` with the highest fidelity. An example is shown below.\n",
        "\n",
        "``` python\n",
        "import numpy as np\n",
        "def obj_func(X):\n",
        "\treturn (-np.array(conduct_experiment(X))) #negative placed if optimisation performed is minimisation\n",
        "\n",
        "X_initial = np.array([[33, 6.25, 10, 20, 20, 0],\n",
        "                      [38, 8, 20, 10, 20, 0]])\n",
        "Y_initial = conduct_experiment(X_initial)\n",
        "print(Y_initial)\n",
        "```\n",
        "\n",
        "#### Goal and Submission\n",
        "Your goal is to develop a Bayesian Optimisation class to obtain the set of inputs which **maximizes the titre at the highest fideility**. You have a **budget of 15000** (observe the cost of running each fidelity) and starting with a maximum of 7 training points that is not a part of the budget. (Remember, you have to have at least 2 points for each variable for the covariance matrix to be calculated.)\n",
        "\n",
        "Please submit your BO class (and GP class) along with the execution block as a .py file to the instructor. A different cell type (with different simulation parameters and maxima) will be used for scoring.\n",
        "\n",
        "This hackathon will be scored based on maximum titre concentration obtained at the highest fidelity. You must stay within the allocated budget! This will be checked, and if exceeded, your submission will be disqualified!\n",
        "\n",
        "#### Form of the BO class and execution block\n",
        "You are allowed to write your own BO class or make modifications to any of the previously seen BO classes.\n",
        "\n",
        "You must include the attributes `self.X` and `self.Y` corresponding to all of your evaluated inputs and outputs as this will be used to retrive the information used for scoring.\n",
        "\n",
        "```python\n",
        "#submission should look something like the following\n",
        "class GP: #if you have any separate classes other than the BO class\n",
        "    def __init__(self, ...):\n",
        "        ...\n",
        "#BO class\n",
        "class BO:\n",
        "    def __init__(self, ...):\n",
        "        self.X = #training data which the evaluated data is to be appended\n",
        "        self.Y = #evaluated via the objective function using self.X\n",
        "\n",
        "# BO Execution Block\n",
        "X_training = [...]\n",
        "X_seachspace = [...]\n",
        "\n",
        "BO_m = BO(...)\n",
        "```\n",
        "\n",
        "#### Guidance (Intermediate - Multi-batch Bayesian Optimsation)\n",
        "You can construct a single-sequential or batch BO algorithm to perform the optimisation. The lowest fidelity experiments do not offer accurate outcomes and you have to choose how many number of expeirments for each fidelity to be performed such that you do not exceed your allocated budget. To link between each fideility, one could perform optimisation on the lower fidilities and then translate the best input conditions to run the highest fidelity experiment.\n",
        "\n",
        "#### Guidance (Advanced - Multi-fidelity Bayesian Optimisation)\n",
        "You can develop a multi-fidelity Bayesian Optimisation algorithm to perform the optimisation. Since the score is based on the highest titre concentration of the highest fidelity, it might be beneficial if you constrain (at least) the last experiment to be run with the highest fidelity - this mitigates the risk that your algorithm does not perform any experiments with the highest fidelity. A basic MFBO algorithm could be created by modifying the acquisition function to one that is cost aware. For example: we have previously used Lower Confidence Bound to balance exploration and exploitation of the search space (see notebook section C). To make this cost aware, we can scale the values obtained from LCB by the cost.\n",
        "\n",
        "```python\n",
        "    def MF_lower_confidence_bound(...):\n",
        "        lower_std = Ysearchspace_mean - acquisition_hyperparam[0]*np.sqrt(Ysearchspace_std)\n",
        "        # mf_lower_std = lower_std / assocated cost for each simulation\n",
        "        return (X_searchspace[np.argmin(mf_lower_std)])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d57ac5a5",
      "metadata": {
        "id": "d57ac5a5"
      },
      "source": [
        "#### Feedback and Scoring Example\n",
        "Once your algorithm is submitted to the instructor, you can request for some feedback on the performance of your algorithm. The final score will be calculated based on the maximum titre concentration obtained from the highest fidelity. 3 plots will be produced to showcase the performance of your algorithm and the performance against the cohort. Example:\n",
        "\n",
        "![image.png](attachment:image.png)\n",
        "![image-2.png](attachment:image-2.png)\n",
        "![image-3.png](attachment:image-3.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bd73484-4185-4126-8d42-fd3b5258cc61",
      "metadata": {
        "id": "4bd73484-4185-4126-8d42-fd3b5258cc61"
      },
      "source": [
        "#### Package Imports\n",
        "\n",
        "Packages are limited to the the ones listed in the package cell - Talk to one of the intructors to ask if it is possible to import other packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "efc60248-a788-4874-9922-36c336a69fcd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efc60248-a788-4874-9922-36c336a69fcd",
        "outputId": "4e7b18aa-9647-4e43-9f7a-daf4d206d15a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sobol_seq\n",
            "  Downloading sobol_seq-0.2.0-py3-none-any.whl.metadata (273 bytes)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sobol_seq) (1.16.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from sobol_seq) (2.0.2)\n",
            "Downloading sobol_seq-0.2.0-py3-none-any.whl (9.2 kB)\n",
            "Installing collected packages: sobol_seq\n",
            "Successfully installed sobol_seq-0.2.0\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (5.24.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly) (9.1.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from plotly) (26.0)\n",
            "Collecting gpytorch\n",
            "  Downloading gpytorch-1.15.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting jaxtyping (from gpytorch)\n",
            "  Downloading jaxtyping-0.3.7-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: mpmath<=1.3,>=0.19 in /usr/local/lib/python3.12/dist-packages (from gpytorch) (1.3.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from gpytorch) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from gpytorch) (1.16.3)\n",
            "Collecting linear_operator>=0.6 (from gpytorch)\n",
            "  Downloading linear_operator-0.6-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.12/dist-packages (from linear_operator>=0.6->gpytorch) (2.9.0+cpu)\n",
            "Requirement already satisfied: numpy<2.6,>=1.25.2 in /usr/local/lib/python3.12/dist-packages (from scipy>=1.6.0->gpytorch) (2.0.2)\n",
            "Collecting wadler-lindig>=0.1.3 (from jaxtyping->gpytorch)\n",
            "  Downloading wadler_lindig-0.1.7-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->gpytorch) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->gpytorch) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->linear_operator>=0.6->gpytorch) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->linear_operator>=0.6->gpytorch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->linear_operator>=0.6->gpytorch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->linear_operator>=0.6->gpytorch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->linear_operator>=0.6->gpytorch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->linear_operator>=0.6->gpytorch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->linear_operator>=0.6->gpytorch) (2025.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0->linear_operator>=0.6->gpytorch) (3.0.3)\n",
            "Downloading gpytorch-1.15.1-py3-none-any.whl (287 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.8/287.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading linear_operator-0.6-py3-none-any.whl (176 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.3/176.3 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxtyping-0.3.7-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.1/56.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wadler_lindig-0.1.7-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: wadler-lindig, jaxtyping, linear_operator, gpytorch\n",
            "Successfully installed gpytorch-1.15.1 jaxtyping-0.3.7 linear_operator-0.6 wadler-lindig-0.1.7\n",
            "Collecting rdkit\n",
            "  Downloading rdkit-2025.9.4-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rdkit) (2.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from rdkit) (11.3.0)\n",
            "Downloading rdkit-2025.9.4-cp312-cp312-manylinux_2_28_x86_64.whl (36.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.6/36.6 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rdkit\n",
            "Successfully installed rdkit-2025.9.4\n",
            "Collecting botorch\n",
            "  Downloading botorch-0.16.1-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from botorch) (4.15.0)\n",
            "Collecting pyre_extensions (from botorch)\n",
            "  Downloading pyre_extensions-0.0.32-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: gpytorch>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from botorch) (1.15.1)\n",
            "Requirement already satisfied: linear_operator>=0.6 in /usr/local/lib/python3.12/dist-packages (from botorch) (0.6)\n",
            "Requirement already satisfied: torch>=2.0.1 in /usr/local/lib/python3.12/dist-packages (from botorch) (2.9.0+cpu)\n",
            "Collecting pyro-ppl>=1.8.4 (from botorch)\n",
            "  Downloading pyro_ppl-1.9.1-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from botorch) (1.16.3)\n",
            "Requirement already satisfied: multipledispatch in /usr/local/lib/python3.12/dist-packages (from botorch) (1.0.0)\n",
            "Requirement already satisfied: threadpoolctl in /usr/local/lib/python3.12/dist-packages (from botorch) (3.6.0)\n",
            "Requirement already satisfied: jaxtyping in /usr/local/lib/python3.12/dist-packages (from gpytorch>=1.14.2->botorch) (0.3.7)\n",
            "Requirement already satisfied: mpmath<=1.3,>=0.19 in /usr/local/lib/python3.12/dist-packages (from gpytorch>=1.14.2->botorch) (1.3.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from gpytorch>=1.14.2->botorch) (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.12/dist-packages (from pyro-ppl>=1.8.4->botorch) (2.0.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from pyro-ppl>=1.8.4->botorch) (3.4.0)\n",
            "Collecting pyro-api>=0.1.1 (from pyro-ppl>=1.8.4->botorch)\n",
            "  Downloading pyro_api-0.1.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.12/dist-packages (from pyro-ppl>=1.8.4->botorch) (4.67.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.1->botorch) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.1->botorch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.1->botorch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.1->botorch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.1->botorch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.1->botorch) (2025.3.0)\n",
            "Collecting typing-inspect (from pyre_extensions->botorch)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: wadler-lindig>=0.1.3 in /usr/local/lib/python3.12/dist-packages (from jaxtyping->gpytorch>=1.14.2->botorch) (0.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.1->botorch) (3.0.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->gpytorch>=1.14.2->botorch) (1.5.3)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect->pyre_extensions->botorch)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading botorch-0.16.1-py3-none-any.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyro_ppl-1.9.1-py3-none-any.whl (755 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyre_extensions-0.0.32-py3-none-any.whl (12 kB)\n",
            "Downloading pyro_api-0.1.2-py3-none-any.whl (11 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: pyro-api, mypy-extensions, typing-inspect, pyro-ppl, pyre_extensions, botorch\n",
            "Successfully installed botorch-0.16.1 mypy-extensions-1.1.0 pyre_extensions-0.0.32 pyro-api-0.1.2 pyro-ppl-1.9.1 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "# if using google collab, run the following pip installs!\n",
        "!pip install sobol_seq\n",
        "!pip install plotly\n",
        "!pip install gpytorch\n",
        "!pip install rdkit\n",
        "!pip install botorch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "977ab0af-7637-40a7-ab13-e7c3491d0477",
      "metadata": {
        "id": "977ab0af-7637-40a7-ab13-e7c3491d0477"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import numpy.random as rnd\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import axes3d, Axes3D\n",
        "import plotly.graph_objs as go\n",
        "from scipy.integrate import quad\n",
        "from scipy.spatial.distance import cdist\n",
        "from scipy.optimize import minimize, differential_evolution, NonlinearConstraint\n",
        "from sklearn.decomposition import PCA\n",
        "import math\n",
        "import time\n",
        "import sobol_seq\n",
        "import torch\n",
        "import gpytorch\n",
        "import copy\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d38720ec",
      "metadata": {
        "id": "d38720ec"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "reactor_list = [\"3LBATCH\", \"3LCONTBATCH\", \"15LCONTBATCH\"]\n",
        "process_parameters = {\n",
        "    \"3LBATCH\": {\n",
        "        \"celltype_1\": {\"my_max\": 0.035, \"K_lysis\": 4e-2,   \"k\": [1e-3, 1e-2, 1e-2],          \"K\": [150, 40, 1, 0.22],    \"Y\": [9.23e7, 8.8e8, 1.6, 0.68, 6.2292e-8, 4.41e-6],    \"m\": [8e-13, 3e-12], \"A\": 1e1, \"pH_opt\": 7.2, \"E_a\": 32},\n",
        "\n",
        "    },\n",
        "    \"3LCONTBATCH\": {\n",
        "        \"celltype_1\": {\"my_max\": 0.035, \"K_lysis\": 4e-2,   \"k\": [1e-3, 1e-2, 1e-2],          \"K\": [150, 40, 1, 0.22],    \"Y\": [9.23e7, 8.8e8, 1.6, 0.68, 6.2292e-8, 4.41e-6],    \"m\": [8e-13, 3e-12], \"A\": 1e1, \"pH_opt\": 7.2, \"E_a\": 32},\n",
        "\n",
        "    },\n",
        "    \"15LCONTBATCH\": {\n",
        "        \"celltype_1\": {\"my_max\": 0.035, \"K_lysis\": 4e-2,   \"k\": [1e-3, 1e-2, 1e-2],          \"K\": [150, 40, 1, 0.22],    \"Y\": [9.23e7, 8.8e8, 1.6, 0.68, 6.2292e-8, 4.41e-6],    \"m\": [8e-13, 3e-12], \"A\": 1e1, \"pH_opt\": 7.2, \"E_a\": 32},\n",
        "\n",
        "    }\n",
        "}\n",
        "NOISE_LEVEL = {\n",
        "            \"3LBATCH\": 2e-1,\n",
        "            \"3LCONTBATCH\": 8e-2,\n",
        "            \"15LCONTBATCH\": 8e-5\n",
        "        }\n",
        "fidelity_cost = {\n",
        "            \"3LBATCH\": 0.05,\n",
        "            \"3LCONTBATCH\": 0.5,\n",
        "            \"15LCONTBATCH\": 1\n",
        "        }\n",
        "data = []\n",
        "for reactor, cell_data in process_parameters.items():\n",
        "    for cell_type, params in cell_data.items():\n",
        "        entry = {\n",
        "            \"reactor\": reactor,\n",
        "            \"cell_type\": cell_type,\n",
        "            **params\n",
        "        }\n",
        "        data.append(entry)\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from scipy.integrate import solve_ivp\n",
        "# import C_Bioprocess_Utils.conditions_data as data\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "class EXPERIMENT:\n",
        "    def __init__(\n",
        "            self,\n",
        "            T: float = 32,\n",
        "            pH: float = 7.2,\n",
        "            cell_type: str = \"celltype_1\",\n",
        "            reactor: str = \"3LBATCH\",\n",
        "            feeding: list = [(10, 0), (20, 0), (30, 0)],\n",
        "            time=150,\n",
        "            df =df\n",
        "        ):\n",
        "\n",
        "        df = df\n",
        "        params = df[(df['reactor'] == reactor) & (df['cell_type'] == cell_type)]\n",
        "        self.reactor    = reactor\n",
        "        self.volume     = 3\n",
        "        self.cell_type  = cell_type\n",
        "        self.time       = time\n",
        "        self.my_max     = params[\"my_max\"].iloc[0]\n",
        "        self.K_lysis    = params[\"K_lysis\"].iloc[0]\n",
        "        self.K_L, self.K_A, self.K_G, self.K_Q = params[\"K\"].iloc[0]\n",
        "        self.Y = params[\"Y\"].iloc[0]\n",
        "        self.m = params[\"m\"].iloc[0]\n",
        "        self.k_d_Q, self.k_d_max, self.k_my = params[\"k\"].iloc[0]\n",
        "\n",
        "        self.A      = params[\"A\"].iloc[0]\n",
        "        self.E_a    = params[\"E_a\"].iloc[0]\n",
        "        self.pH_opt = params[\"pH_opt\"].iloc[0]\n",
        "\n",
        "        self.initial_conditions = [0, 1e6, 0.8 * 1e6, 0, 210, 1, 9, 0]\n",
        "        self.solution = None\n",
        "        self.t = None\n",
        "        self.T         = T\n",
        "        self.pH        = pH\n",
        "\n",
        "        self.feeding = feeding\n",
        "\n",
        "        self.R = 8.314\n",
        "\n",
        "    def temperature_effect(self):\n",
        "        x = self.T\n",
        "        mu = self.E_a\n",
        "        A = 5\n",
        "\n",
        "        left_part = np.exp(-1 * ((x - mu) / 10)**2)\n",
        "        right_part = np.exp(-0.9 * ((x - mu) / 3.6)**2)\n",
        "\n",
        "        factor = A * np.where(x < mu, left_part, right_part)\n",
        "        return factor\n",
        "\n",
        "\n",
        "    def pH_effect(self) -> float:\n",
        "        x = self.pH\n",
        "        mu = self.pH_opt\n",
        "        A = 2\n",
        "\n",
        "        left_part = np.exp(-0.8 * ((x - mu) / 1)**2)\n",
        "        right_part = np.exp(-1 * ((x - mu) / 0.5)**2)\n",
        "\n",
        "        factor = A * np.where(x < mu, left_part, right_part)\n",
        "        return factor\n",
        "\n",
        "    def my(self, G, Q, L, A):\n",
        "        temperature_factor = self.temperature_effect()\n",
        "        pH_factor = self.pH_effect()\n",
        "\n",
        "        my_max = self.my_max\n",
        "        K_G = self.K_G\n",
        "        K_Q = self.K_Q\n",
        "        K_L = self.K_L\n",
        "        K_A = self.K_A\n",
        "\n",
        "        my = my_max * G/(K_G + G) * Q/(K_Q + Q) * K_L/(K_L + L) * K_A/(K_A + A) * temperature_factor * pH_factor\n",
        "        return my\n",
        "\n",
        "    def ODE(self,t,x):\n",
        "        P, X_T, X_V, X_D, G, Q, L, A = x\n",
        "        my = self.my(G, Q, L, A)\n",
        "        k_d = self.k_d_max * (self.k_my/(my + self.k_my))\n",
        "        K_lysis = self.K_lysis\n",
        "        k_d_Q = self.k_d_Q\n",
        "        K_G = self.K_G\n",
        "\n",
        "        Y_X_G, Y_X_Q, Y_L_G, Y_A_Q, Y_P_X, Y_dot_P_X = self.Y\n",
        "        m_G, m_Q = self.m\n",
        "\n",
        "        dX_T_dt = my * X_V - K_lysis * X_D\n",
        "        dX_V_dt = (my-k_d) * X_V\n",
        "        dX_D_dt = k_d * X_V - K_lysis * X_D\n",
        "\n",
        "        dP_dt = Y_P_X * X_T + Y_dot_P_X * (my * G / (K_G + G)) * X_V\n",
        "\n",
        "        dG_dt = X_V * (-my/Y_X_G - m_G)\n",
        "        dQ_dt = X_V * (-my/Y_X_Q - m_Q) - k_d_Q * Q\n",
        "        dL_dt = -X_V * Y_L_G * (-my/Y_X_G - m_G)\n",
        "        dA_dt = -X_V * Y_A_Q * (-my/Y_X_Q - m_Q) + k_d_Q * Q\n",
        "\n",
        "        gradients = [dP_dt, dX_T_dt, dX_V_dt, dX_D_dt, dG_dt, dQ_dt, dL_dt, dA_dt]\n",
        "\n",
        "        return gradients\n",
        "\n",
        "    def ODE_solver(self):\n",
        "        t_span = (0, self.time)\n",
        "        t_eval_total = []\n",
        "        y_total = []\n",
        "        current_t = 0\n",
        "        current_y = self.initial_conditions.copy()\n",
        "\n",
        "        for event_time, new_G_value in self.feeding:\n",
        "            t_span_segment = (current_t, event_time)\n",
        "            t_eval_segment = np.linspace(current_t, event_time, 1000)\n",
        "\n",
        "            solution = solve_ivp(\n",
        "                fun=self.ODE,\n",
        "                t_span=t_span_segment,\n",
        "                y0=current_y,\n",
        "                t_eval=t_eval_segment,\n",
        "                method=\"RK45\"\n",
        "            )\n",
        "\n",
        "            t_eval_total.extend(solution.t)\n",
        "            y_total.append(solution.y)\n",
        "\n",
        "            current_t = event_time\n",
        "            current_y = solution.y[:, -1]\n",
        "            if current_y[4] < new_G_value:\n",
        "                current_y[4] = new_G_value\n",
        "\n",
        "            new_Q_value = new_G_value * 0.4\n",
        "            if current_y[5] < new_Q_value:\n",
        "                current_y[5] = new_Q_value\n",
        "\n",
        "        t_span_segment = (current_t, self.time)\n",
        "        t_eval_segment = np.linspace(current_t, self.time, 500)\n",
        "\n",
        "        solution = solve_ivp(\n",
        "            fun=self.ODE,\n",
        "            t_span=t_span_segment,\n",
        "            y0=current_y,\n",
        "            t_eval=t_eval_segment,\n",
        "            method=\"RK45\"\n",
        "        )\n",
        "\n",
        "        t_eval_total.extend(solution.t)\n",
        "        y_total.append(solution.y)\n",
        "\n",
        "        t_eval_total = np.array(t_eval_total)\n",
        "        y_total = np.hstack(y_total)\n",
        "\n",
        "        self.solution = y_total\n",
        "        self.solution[0] = self.solution[0] / (self.volume * 1e3) # Transform unit into g/L\n",
        "\n",
        "        self.t = t_eval_total\n",
        "        return y_total\n",
        "\n",
        "    def measurement(self, noise_level=None, quantity=\"P\"):\n",
        "        # NOTE: this makes every call produce the same noise\n",
        "        # If you want different noise each call, remove this line.\n",
        "        np.random.seed(1234)\n",
        "\n",
        "        reactor_type = self.reactor\n",
        "\n",
        "        # noise_level can be:\n",
        "        #  - None (use default map)\n",
        "        #  - dict (map per reactor)\n",
        "        #  - number (use directly)\n",
        "        if noise_level is None:\n",
        "            noise_level = NOISE_LEVEL.get(reactor_type, None)\n",
        "            if noise_level is None:\n",
        "                raise ValueError(f\"Unknown reactor type: {reactor_type}\")\n",
        "        elif isinstance(noise_level, dict):\n",
        "            noise_level = noise_level.get(reactor_type, None)\n",
        "            if noise_level is None:\n",
        "                raise ValueError(f\"Unknown reactor type in provided noise map: {reactor_type}\")\n",
        "        else:\n",
        "            noise_level = float(noise_level)\n",
        "\n",
        "        self.ODE_solver()\n",
        "\n",
        "        index = {\"P\": 0, \"X_T\": 1, \"X_V\": 2, \"X_D\": 3, \"G\": 4, \"Q\": 5, \"L\": 6, \"A\": 7}\n",
        "        true_value = self.solution[index[quantity]][-1]\n",
        "\n",
        "        noise_magnitude = max(noise_level * true_value, 1e-8)\n",
        "        noise = np.random.normal(0, noise_magnitude)\n",
        "        return true_value + noise\n",
        "\n",
        "\n",
        "def conduct_experiment(X, initial_conditions: list = [0, 0.4 * 1e9, 0.4 * 1e6, 0, 20, 3.5, 0, 1.8], noise_level=None):\n",
        "    result = []\n",
        "    feeding = [(10, 0), (20, 0), (30, 0)]\n",
        "    reactor = \"3LBATCH\"\n",
        "\n",
        "    for row in X:\n",
        "        if len(row) == 2:\n",
        "            T, pH = row\n",
        "        elif len(row) == 5:\n",
        "            T, pH, F1, F2, F3 = row\n",
        "            feeding = [(40, float(F1)), (80, float(F2)), (120, float(F3))]\n",
        "        elif len(row) == 6:\n",
        "            T, pH, F1, F2, F3, fidelity = row\n",
        "            if np.round(fidelity) == 0:\n",
        "                feeding = [(40, 0), (60, float(F2)), (120, 0)]\n",
        "            else:\n",
        "                feeding = [(40, float(F1)), (80, float(F2)), (120, float(F3))]\n",
        "            reactor = reactor_list[int(np.round(fidelity))]\n",
        "        else:\n",
        "            raise ValueError(f\"Cannot handle the dimensionality of X. n must be 2, 5 or 6 but is {len(row)}\")\n",
        "\n",
        "        cell = EXPERIMENT(T=T, pH=pH, time=150, feeding=feeding, reactor=reactor)\n",
        "        cell.initial_conditions = initial_conditions\n",
        "        value = float(cell.measurement(quantity=\"P\", noise_level=noise_level))\n",
        "        #print(value)\n",
        "        result.append(value)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "eae15efe-9687-418d-9b4d-32031d09ae78",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eae15efe-9687-418d-9b4d-32031d09ae78",
        "outputId": "48bc164d-d465-4c92-92cd-356b7b8b5b3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10.355266633865073, 1.3630867118273515]\n"
          ]
        }
      ],
      "source": [
        "# Check if this runs without errors!\n",
        "\n",
        "def obj_func(X):\n",
        "\treturn (-np.array(conduct_experiment(X))) #negative placed if optimisation performed is minimisation\n",
        "\n",
        "X_initial = np.array([[33, 6.25, 10, 20, 20, 0],\n",
        "                      [38, 8, 20, 10, 20, 0]])\n",
        "Y_initial = conduct_experiment(X_initial)\n",
        "print(Y_initial)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "751ac447",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "751ac447",
        "outputId": "339aa704-d4a0-4b53-90a6-92d9808916f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cheap Result: [10.240281638386119] (Cost: 10)\n",
            "Expensive Result: [13.328600225623239] (Cost: 2100)\n"
          ]
        }
      ],
      "source": [
        "# Let's try a LOW fidelity run (Last number is 0)\n",
        "# Temp=33, pH=7.0, F1=10, F2=10, F3=10, Fid=0\n",
        "X_cheap = np.array([[33, 7.0, 10, 10, 10, 0]])\n",
        "Y_cheap = conduct_experiment(X_cheap)\n",
        "print(f\"Cheap Result: {Y_cheap} (Cost: 10)\")\n",
        "\n",
        "# Let's try a HIGH fidelity run (Last number is 2)\n",
        "# Same inputs, just better machine\n",
        "X_expensive = np.array([[33, 7.0, 10, 10, 10, 2]])\n",
        "Y_expensive = conduct_experiment(X_expensive)\n",
        "print(f\"Expensive Result: {Y_expensive} (Cost: 2100)\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BO:\n",
        "    def __init__(self, bounds, budget=15000):\n",
        "        # 1. Setup the \"Memory\"\n",
        "        self.X = [] # Stores inputs: [Temp, pH, F1, F2, F3, Fidelity]\n",
        "        self.Y = [] # Stores outputs: [Titre]\n",
        "\n",
        "        # 2. Setup the \"Wallet\"\n",
        "        self.budget = budget\n",
        "        self.bounds = bounds\n",
        "        self.iteration = 0  # Starts at 0\n",
        "\n",
        "        # 3. Cost Menu (from the problem description)\n",
        "        self.cost_map = {0: 10, 1: 575, 2: 2100}\n",
        "\n",
        "    def update_data(self, x_new, y_new):\n",
        "        \"\"\"Adds new experiment data and deducts the cost.\"\"\"\n",
        "        self.X.append(x_new)\n",
        "        self.Y.append(y_new)\n",
        "\n",
        "        # Identify fidelity (the last column of x_new)\n",
        "        fidelity = int(x_new[-1])\n",
        "        cost = self.cost_map[fidelity]\n",
        "\n",
        "        self.budget -= cost\n",
        "\n",
        "        # Update Clock\n",
        "        self.iteration += 1 # Counts up by 1\n",
        "\n",
        "        # Format the recipe for easy reading\n",
        "        recipe = f\"T={x_new[0]:.1f}, pH={x_new[1]:.1f}, Feeds=[{x_new[2]:.0f}, {x_new[3]:.0f}, {x_new[4]:.0f}]\"\n",
        "\n",
        "        # Print the full status\n",
        "        print(f\"ITERATION {self.iteration}:\")\n",
        "        print(f\"   -> Machine Used: Fidelity {fidelity} (Cost: {cost})\")\n",
        "        print(f\"   -> Recipe Tried: {recipe}\")\n",
        "        print(f\"   -> RESULT (Y):   {y_new[0]:.4f} g/L\") # [cite: 7]\n",
        "        print(f\"   -> Budget Left:  {self.budget}\\n\")\n",
        "\n",
        "    def suggest_next_point(self):\n",
        "        \"\"\"\n",
        "        PLACEHOLDER: Currently acts as a 'Random Search'.\n",
        "        Later, you will replace this with your Gaussian Process logic.\n",
        "        \"\"\"\n",
        "        # Generate random inputs within bounds\n",
        "        # Temp (30-40), pH (6-8), Feeds (0-50)\n",
        "        next_x = [\n",
        "            np.random.uniform(self.bounds[0,0], self.bounds[0,1]),\n",
        "            np.random.uniform(self.bounds[1,0], self.bounds[1,1]),\n",
        "            np.random.uniform(self.bounds[2,0], self.bounds[2,1]),\n",
        "            np.random.uniform(self.bounds[3,0], self.bounds[3,1]),\n",
        "            np.random.uniform(self.bounds[4,0], self.bounds[4,1]),\n",
        "            # Randomly choose a fidelity (0, 1, or 2) for now\n",
        "            np.random.randint(0, 3)\n",
        "        ]\n",
        "        return np.array(next_x)"
      ],
      "metadata": {
        "id": "Dy4nQiTXbZyh"
      },
      "id": "Dy4nQiTXbZyh",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Initialize\n",
        "# Bounds: Temp(30-40), pH(6-8), F1(0-50), F2(0-50), F3(0-50), Fid(0-2)\n",
        "bounds = np.array([\n",
        "    [30, 40], # Temperature\n",
        "    [6, 8],   # pH\n",
        "    [0, 50],  # Feed 1\n",
        "    [0, 50],  # Feed 2\n",
        "    [0, 50],  # Feed 3\n",
        "    [0, 2]    # Fidelity (0=Low, 1=Mid, 2=High)\n",
        "])\n",
        "optimizer = BO(bounds)\n",
        "\n",
        "print(\"--- Starting Random Warmup ---\")\n",
        "\n",
        "# 2. Random Warmup (Run 5 cheap experiments to start)\n",
        "for _ in range(5):\n",
        "    # Create random cheap input [Temp, pH, F1, F2, F3, 0]\n",
        "    x_random = np.array([np.random.uniform(30, 40), # Temp\n",
        "        np.random.uniform(6, 8),   # pH\n",
        "        np.random.uniform(0, 50),  # F1\n",
        "        np.random.uniform(0, 50),  # F2\n",
        "        np.random.uniform(0, 50),  # F3\n",
        "        0])                        # Fidelity=0 for cheap Warmup Loop\n",
        "    y_random = conduct_experiment([x_random])\n",
        "    optimizer.update_data(x_random, y_random)\n",
        "\n",
        "print(\"\\n--- Starting Main Optimization Loop ---\")\n",
        "\n",
        "# 3. The Main Loop\n",
        "while optimizer.budget > 10: # While we can afford at least a cheap run\n",
        "\n",
        "    # A. Ask the Brain for the next best recipe\n",
        "    x_next = optimizer.suggest_next_point()\n",
        "\n",
        "    # B. Check if we can afford it\n",
        "    fidelity = int(x_next[-1])\n",
        "    cost = optimizer.cost_map[fidelity]\n",
        "\n",
        "    if cost > optimizer.budget:\n",
        "        print(\"Cannot afford this fidelity anymore! Stopping or switching to cheap.\")\n",
        "        break\n",
        "\n",
        "    # C. Run the experiment\n",
        "    y_next = conduct_experiment([x_next]) # [cite: 18]\n",
        "\n",
        "    # D. Learn from it\n",
        "    optimizer.update_data(x_next, y_next)\n",
        "\n",
        "print(\"Optimization Done! Best Result:\", max(optimizer.Y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRnpOS-_biaN",
        "outputId": "d05c3930-797a-40c1-9e5a-da4f77f7de06"
      },
      "id": "qRnpOS-_biaN",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Random Warmup ---\n",
            "ITERATION 1:\n",
            "   -> Machine Used: Fidelity 0 (Cost: 10)\n",
            "   -> Recipe Tried: T=39.6, pH=7.8, Feeds=[18, 25, 34]\n",
            "   -> RESULT (Y):   1.3630 g/L\n",
            "   -> Budget Left:  14990\n",
            "\n",
            "ITERATION 2:\n",
            "   -> Machine Used: Fidelity 0 (Cost: 10)\n",
            "   -> Recipe Tried: T=34.4, pH=7.6, Feeds=[39, 14, 14]\n",
            "   -> RESULT (Y):   6.8932 g/L\n",
            "   -> Budget Left:  14980\n",
            "\n",
            "ITERATION 3:\n",
            "   -> Machine Used: Fidelity 0 (Cost: 10)\n",
            "   -> Recipe Tried: T=34.4, pH=7.6, Feeds=[39, 14, 14]\n",
            "   -> RESULT (Y):   6.8932 g/L\n",
            "   -> Budget Left:  14970\n",
            "\n",
            "ITERATION 4:\n",
            "   -> Machine Used: Fidelity 0 (Cost: 10)\n",
            "   -> Recipe Tried: T=34.4, pH=7.6, Feeds=[39, 14, 14]\n",
            "   -> RESULT (Y):   6.8932 g/L\n",
            "   -> Budget Left:  14960\n",
            "\n",
            "ITERATION 5:\n",
            "   -> Machine Used: Fidelity 0 (Cost: 10)\n",
            "   -> Recipe Tried: T=34.4, pH=7.6, Feeds=[39, 14, 14]\n",
            "   -> RESULT (Y):   6.8932 g/L\n",
            "   -> Budget Left:  14950\n",
            "\n",
            "\n",
            "--- Starting Main Optimization Loop ---\n",
            "ITERATION 6:\n",
            "   -> Machine Used: Fidelity 2 (Cost: 2100)\n",
            "   -> Recipe Tried: T=34.4, pH=7.6, Feeds=[39, 14, 14]\n",
            "   -> RESULT (Y):   15.9893 g/L\n",
            "   -> Budget Left:  12850\n",
            "\n",
            "ITERATION 7:\n",
            "   -> Machine Used: Fidelity 2 (Cost: 2100)\n",
            "   -> Recipe Tried: T=34.4, pH=7.6, Feeds=[39, 14, 14]\n",
            "   -> RESULT (Y):   15.9893 g/L\n",
            "   -> Budget Left:  10750\n",
            "\n",
            "ITERATION 8:\n",
            "   -> Machine Used: Fidelity 2 (Cost: 2100)\n",
            "   -> Recipe Tried: T=34.4, pH=7.6, Feeds=[39, 14, 14]\n",
            "   -> RESULT (Y):   15.9893 g/L\n",
            "   -> Budget Left:  8650\n",
            "\n",
            "ITERATION 9:\n",
            "   -> Machine Used: Fidelity 2 (Cost: 2100)\n",
            "   -> Recipe Tried: T=34.4, pH=7.6, Feeds=[39, 14, 14]\n",
            "   -> RESULT (Y):   15.9893 g/L\n",
            "   -> Budget Left:  6550\n",
            "\n",
            "ITERATION 10:\n",
            "   -> Machine Used: Fidelity 2 (Cost: 2100)\n",
            "   -> Recipe Tried: T=34.4, pH=7.6, Feeds=[39, 14, 14]\n",
            "   -> RESULT (Y):   15.9893 g/L\n",
            "   -> Budget Left:  4450\n",
            "\n",
            "ITERATION 11:\n",
            "   -> Machine Used: Fidelity 2 (Cost: 2100)\n",
            "   -> Recipe Tried: T=34.4, pH=7.6, Feeds=[39, 14, 14]\n",
            "   -> RESULT (Y):   15.9893 g/L\n",
            "   -> Budget Left:  2350\n",
            "\n",
            "ITERATION 12:\n",
            "   -> Machine Used: Fidelity 2 (Cost: 2100)\n",
            "   -> Recipe Tried: T=34.4, pH=7.6, Feeds=[39, 14, 14]\n",
            "   -> RESULT (Y):   15.9893 g/L\n",
            "   -> Budget Left:  250\n",
            "\n",
            "Cannot afford this fidelity anymore! Stopping or switching to cheap.\n",
            "Optimization Done! Best Result: [15.989348692716073]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import gpytorch\n",
        "from botorch.models import SingleTaskGP\n",
        "from botorch.models.transforms import Normalize\n",
        "from botorch.fit import fit_gpytorch_mll\n",
        "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
        "\n",
        "# ==========================================\n",
        "# 1. THE GP CLASS (The \"Brain\")\n",
        "# ==========================================\n",
        "class GP:\n",
        "    def __init__(self, train_x, train_y):\n",
        "        \"\"\"\n",
        "        Wraps the BoTorch/GPyTorch model logic.\n",
        "        \"\"\"\n",
        "        # Convert numpy arrays to Torch Tensors (required for BoTorch)\n",
        "        self.train_x = torch.tensor(train_x).double()\n",
        "        self.train_y = torch.tensor(train_y).double()\n",
        "\n",
        "        # Check if Y is flat (dim=1). If so, make it 2D (N, 1).\n",
        "        if self.train_y.ndim == 1:\n",
        "            self.train_y = self.train_y.unsqueeze(-1)\n",
        "\n",
        "# 2. FIX SCALING: Normalize inputs to [0, 1] range\n",
        "        # This silences the \"InputDataWarning\" and improves accuracy\n",
        "        # We assume 6 input dimensions (Temp, pH, F1, F2, F3, Fid)\n",
        "        # Initialize the Single Task Gaussian Process with Normalization\n",
        "        self.model = SingleTaskGP(\n",
        "            self.train_x,\n",
        "            self.train_y,\n",
        "            input_transform=Normalize(d=self.train_x.shape[-1]) # Normalization\n",
        "        )\n",
        "\n",
        "        self.mll = ExactMarginalLogLikelihood(self.model.likelihood, self.model)\n",
        "\n",
        "        # Initialize the Single Task Gaussian Process\n",
        "        self.model = SingleTaskGP(self.train_x, self.train_y)\n",
        "        self.mll = ExactMarginalLogLikelihood(self.model.likelihood, self.model)\n",
        "\n",
        "    def fit(self):\n",
        "        \"\"\"Trains the model on the current data.\"\"\"\n",
        "        fit_gpytorch_mll(self.mll)\n",
        "\n",
        "    def predict(self, candidates):\n",
        "        \"\"\"\n",
        "        Predicts Mean and StdDev for a list of candidate inputs.\n",
        "        Returns numpy arrays for easier handling.\n",
        "        \"\"\"\n",
        "        # Set to eval mode for prediction\n",
        "        self.model.eval()\n",
        "        candidate_tensor = torch.tensor(candidates).double()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            posterior = self.model.posterior(candidate_tensor)\n",
        "            mean = posterior.mean.squeeze().numpy()\n",
        "            std = posterior.variance.sqrt().squeeze().numpy()\n",
        "\n",
        "        return mean, std\n",
        "\n",
        "# ==========================================\n",
        "# 2. THE BO CLASS (The \"Optimizer\")\n",
        "# ==========================================\n",
        "class BO:\n",
        "    def __init__(self, bounds, budget=15000):\n",
        "        # REQUIRED ATTRIBUTES\n",
        "        self.X = [] # List to store inputs\n",
        "        self.Y = [] # List to store outputs\n",
        "\n",
        "        self.bounds = bounds\n",
        "        self.budget = budget\n",
        "        self.cost_map = {0: 10, 1: 575, 2: 2100}\n",
        "\n",
        "    def run_initial_warmup(self, n_points=5):\n",
        "        \"\"\"Runs cheap random experiments to get data for the GP.\"\"\"\n",
        "        print(\"--- Starting Warmup (Fidelity 0) ---\")\n",
        "        for _ in range(n_points):\n",
        "            # Generate random cheap point\n",
        "            x_random = [\n",
        "                np.random.uniform(self.bounds[0,0], self.bounds[0,1]), # Temp\n",
        "                np.random.uniform(self.bounds[1,0], self.bounds[1,1]), # pH\n",
        "                np.random.uniform(self.bounds[2,0], self.bounds[2,1]), # F1\n",
        "                np.random.uniform(self.bounds[3,0], self.bounds[3,1]), # F2\n",
        "                np.random.uniform(self.bounds[4,0], self.bounds[4,1]), # F3\n",
        "                0 # Force Low Fidelity\n",
        "            ]\n",
        "            # Run experiment\n",
        "            y_result = conduct_experiment([x_random])[0]\n",
        "\n",
        "            # Store it\n",
        "            self.update_data(x_random, y_result)\n",
        "\n",
        "    def update_data(self, x_new, y_new):\n",
        "        \"\"\"Updates internal memory and budget.\"\"\"\n",
        "        self.X.append(x_new)\n",
        "        self.Y.append(y_new)\n",
        "\n",
        "        fidelity = int(x_new[-1])\n",
        "        cost = self.cost_map[fidelity]\n",
        "        self.budget -= cost\n",
        "\n",
        "        print(f\"   -> Ran Fidelity {fidelity} (Cost: {cost}). Result: {y_new:.4f}. Budget Left: {self.budget}\")\n",
        "\n",
        "    def MF_lower_confidence_bound(self, gp_model, n_candidates=5000):\n",
        "        \"\"\"\n",
        "        ADVANCED GUIDANCE IMPLEMENTATION:\n",
        "        Calculates LCB and divides by Cost to prioritize cheap experiments.\n",
        "        \"\"\"\n",
        "        # 1. Create huge list of random candidates to test\n",
        "        candidates = np.random.uniform(\n",
        "            low=self.bounds[:, 0],\n",
        "            high=self.bounds[:, 1],\n",
        "            size=(n_candidates, 6)\n",
        "        )\n",
        "        # Fix Fidelity column to be integers 0, 1, or 2\n",
        "        candidates[:, -1] = np.random.randint(0, 3, size=n_candidates)\n",
        "\n",
        "        # 2. Ask GP for predictions\n",
        "        mean, std = gp_model.predict(candidates)\n",
        "\n",
        "        # 3. Calculate LCB (Lower Confidence Bound)\n",
        "        # We minimize negative titre.\n",
        "        beta = 1.96\n",
        "        lower_std = mean - (beta * std)\n",
        "\n",
        "        # 4. COST AWARE SCALING (The Magic Step)\n",
        "        # mf_lower_std = lower_std / associated cost\n",
        "        costs = np.array([self.cost_map[int(f)] for f in candidates[:, -1]])\n",
        "        mf_lower_std = lower_std / costs\n",
        "\n",
        "        # 5. Pick the \"most negative\" value\n",
        "        best_index = np.argmin(mf_lower_std)\n",
        "        return candidates[best_index]\n",
        "\n",
        "    def optimize(self):\n",
        "        \"\"\"The Main Loop\"\"\"\n",
        "        self.run_initial_warmup()\n",
        "\n",
        "        while self.budget > 10:\n",
        "            # SAFETY CHECK: If budget is getting low (near 2500),\n",
        "            # force a High Fidelity run and stop.\n",
        "            if self.budget < 2500:\n",
        "                print(\"!!! BUDGET LOW - FORCING FINAL HIGH FIDELITY RUN !!!\")\n",
        "                # Pick best point found so far, but upgrade it to Fidelity 2\n",
        "                best_idx = np.argmin(self.Y) # Minimizing negative = best yield\n",
        "                final_x = list(self.X[best_idx])\n",
        "                final_x[-1] = 2 # Force High Fidelity\n",
        "\n",
        "                # Check if we can afford it (just in case)\n",
        "                if self.budget >= 2100:\n",
        "                    y_res = conduct_experiment([final_x])[0]\n",
        "                    self.update_data(final_x, y_res)\n",
        "                break\n",
        "\n",
        "            # 1. Instantiate and Fit GP\n",
        "            gp = GP(self.X, self.Y)\n",
        "            gp.fit()\n",
        "\n",
        "            # 2. Get next suggestion using Cost-Aware LCB\n",
        "            x_next = self.MF_lower_confidence_bound(gp)\n",
        "\n",
        "            # 3. Check Budget\n",
        "            fidelity = int(x_next[-1])\n",
        "            cost = self.cost_map[fidelity]\n",
        "\n",
        "            if cost > self.budget:\n",
        "                print(\"Too expensive! Skipping this suggestion.\")\n",
        "                break\n",
        "\n",
        "            # 4. Run Experiment\n",
        "            y_next = conduct_experiment([x_next])[0]\n",
        "            self.update_data(x_next, y_next)\n",
        "\n",
        "# ==========================================\n",
        "# 3. BO EXECUTION BLOCK\n",
        "# ==========================================\n",
        "\n",
        "# Define Search Space (Bounds)\n",
        "# Temp(30-40), pH(6-8), F1(0-50), F2(0-50), F3(0-50), Fid(0-2)\n",
        "X_searchspace = np.array([\n",
        "    [30, 40], [6, 8], [0, 50], [0, 50], [0, 50], [0, 2]\n",
        "])\n",
        "\n",
        "# Initialize the Optimizer\n",
        "BO_m = BO(X_searchspace, budget=15000)\n",
        "\n",
        "# Run the Optimization\n",
        "BO_m.optimize()\n",
        "\n",
        "# Final Report\n",
        "print(\"\\n--- OPTIMIZATION COMPLETE ---\")\n",
        "# Remember to convert back to positive for reading (since we minimized negative)\n",
        "best_titre = -1 * min(BO_m.Y)\n",
        "print(f\"Highest Titre Found: {best_titre:.4f} g/L\")"
      ],
      "metadata": {
        "id": "NRi0HNJXoWI4"
      },
      "id": "NRi0HNJXoWI4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import gpytorch\n",
        "from botorch.models import SingleTaskGP\n",
        "from botorch.models.transforms import Normalize\n",
        "from botorch import fit_gpytorch_mll\n",
        "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
        "from scipy.stats import norm # <--- NEW IMPORT FOR MATH\n",
        "\n",
        "# ==========================================\n",
        "# 1. THE GP CLASS\n",
        "# ==========================================\n",
        "class GP:\n",
        "    def __init__(self, train_x, train_y):\n",
        "        self.train_x = torch.tensor(train_x).double()\n",
        "        self.train_y = torch.tensor(train_y).double()\n",
        "        if self.train_y.ndim == 1:\n",
        "            self.train_y = self.train_y.unsqueeze(-1)\n",
        "\n",
        "        self.model = SingleTaskGP(\n",
        "            self.train_x,\n",
        "            self.train_y,\n",
        "            input_transform=Normalize(d=self.train_x.shape[-1])\n",
        "        )\n",
        "        self.mll = ExactMarginalLogLikelihood(self.model.likelihood, self.model)\n",
        "\n",
        "    def fit(self):\n",
        "        fit_gpytorch_mll(self.mll)\n",
        "\n",
        "    def predict(self, candidates):\n",
        "        self.model.eval()\n",
        "        candidate_tensor = torch.tensor(candidates).double()\n",
        "        with torch.no_grad():\n",
        "            posterior = self.model.posterior(candidate_tensor)\n",
        "            mean = posterior.mean.squeeze().numpy()\n",
        "            std = posterior.variance.sqrt().squeeze().numpy()\n",
        "        return mean, std\n",
        "\n",
        "# ==========================================\n",
        "# 2. THE BO CLASS (Fixed with Expected Improvement)\n",
        "# ==========================================\n",
        "class BO:\n",
        "    def __init__(self, bounds, budget=15000):\n",
        "        self.X = []\n",
        "        self.Y = []\n",
        "        self.bounds = bounds\n",
        "        self.budget = budget\n",
        "        self.cost_map = {0: 10, 1: 575, 2: 2100}\n",
        "\n",
        "    def run_initial_warmup(self, n_points=5):\n",
        "        print(\"--- Starting Warmup (Fidelity 0) ---\")\n",
        "        for _ in range(n_points):\n",
        "\n",
        "            x_random = [\n",
        "                np.random.uniform(self.bounds[0,0], self.bounds[0,1]),\n",
        "                np.random.uniform(self.bounds[1,0], self.bounds[1,1]),\n",
        "                np.random.uniform(self.bounds[2,0], self.bounds[2,1]),\n",
        "                np.random.uniform(self.bounds[3,0], self.bounds[3,1]),\n",
        "                np.random.uniform(self.bounds[4,0], self.bounds[4,1]),\n",
        "                0 # Force Low Fidelity\n",
        "            ]\n",
        "\n",
        "            # Run experiment\n",
        "            y_result = conduct_experiment([x_random])[0]\n",
        "\n",
        "            # Store it\n",
        "            self.update_data(x_random, y_result)\n",
        "\n",
        "    def update_data(self, x_new, y_new):\n",
        "        if isinstance(x_new, np.ndarray): x_new = x_new.tolist()\n",
        "        self.X.append(x_new)\n",
        "        # Store negative yield for minimization\n",
        "        self.Y.append(-1 * y_new)\n",
        "\n",
        "        fidelity = int(x_new[-1])\n",
        "        cost = self.cost_map[fidelity]\n",
        "        self.budget -= cost\n",
        "        print(f\"   -> Ran Fidelity {fidelity} (Cost: {cost}). Result: {y_new:.4f} g/L. Budget Left: {self.budget}\")\n",
        "\n",
        "    def MF_expected_improvement(self, gp_model, n_candidates=5000):\n",
        "        \"\"\"\n",
        "        Calculates Expected Improvement (EI) scaled by Cost.\n",
        "        This prevents the AI from getting stuck on the same point.\n",
        "        \"\"\"\n",
        "        # 1. Generate Candidates\n",
        "        candidates = np.random.uniform(low=self.bounds[:, 0], high=self.bounds[:, 1], size=(n_candidates, 6))\n",
        "        candidates[:, -1] = np.random.randint(0, 3, size=n_candidates)\n",
        "\n",
        "        # 2. Predict Mean and Std\n",
        "        mean, std = gp_model.predict(candidates)\n",
        "\n",
        "        # 3. Calculate EI (The Manual Math Way)\n",
        "        # We are minimizing negative titre. Best so far is the lowest value in self.Y\n",
        "        best_f = np.min(self.Y)\n",
        "        xi = 0.01 # Jitter (Exploration parameter)\n",
        "\n",
        "        # Z-score calculation\n",
        "        with np.errstate(divide='warn'):\n",
        "            imp = best_f - mean - xi\n",
        "            Z = imp / std\n",
        "            # If std is 0 (we already visited this point), EI should be 0\n",
        "            ei = imp * norm.cdf(Z) + std * norm.pdf(Z)\n",
        "            ei[std == 0.0] = 0.0\n",
        "\n",
        "        # 4. Cost Scaling (Maximize Bang for Buck)\n",
        "        # EI is positive \"Gain\". We want to Maximize (Gain / Cost).\n",
        "        costs = np.array([self.cost_map[int(f)] for f in candidates[:, -1]])\n",
        "\n",
        "        # We add a tiny epsilon to cost to avoid divide by zero (just in case)\n",
        "        score = ei / costs\n",
        "\n",
        "        # 5. Pick the Winner (Argmax because we want HIGHEST efficiency)\n",
        "        best_index = np.argmax(score)\n",
        "        return candidates[best_index]\n",
        "\n",
        "    def optimize(self):\n",
        "        self.run_initial_warmup()\n",
        "\n",
        "        while self.budget > 10:\n",
        "            # SAFETY CHECK: If budget is low, force High Fid verification\n",
        "            if self.budget < 2500:\n",
        "                print(\"!!! BUDGET LOW - FORCING FINAL HIGH FIDELITY RUN !!!\")\n",
        "                best_idx = np.argmin(self.Y)\n",
        "                final_x = list(self.X[best_idx])\n",
        "\n",
        "                # Only run if we haven't already verified this exact spot at High Fid\n",
        "                if final_x[-1] != 2:\n",
        "                    final_x[-1] = 2\n",
        "                    if self.budget >= 2100:\n",
        "                        y_res = conduct_experiment([final_x])[0]\n",
        "                        self.update_data(final_x, y_res)\n",
        "                break\n",
        "\n",
        "            # 1. Fit GP\n",
        "            gp = GP(self.X, self.Y)\n",
        "            gp.fit()\n",
        "\n",
        "            # 2. Get suggestion using EI / Cost\n",
        "            x_next = self.MF_expected_improvement(gp)\n",
        "\n",
        "            # 3. Check Budget\n",
        "            cost = self.cost_map[int(x_next[-1])]\n",
        "            if cost > self.budget:\n",
        "                print(\"Too expensive! Skipping.\")\n",
        "                break\n",
        "\n",
        "            y_next = conduct_experiment([x_next])[0]\n",
        "            self.update_data(x_next, y_next)\n",
        "\n",
        "# ==========================================\n",
        "# 3. EXECUTION\n",
        "# ==========================================\n",
        "X_searchspace = np.array([[30, 40], [6, 8], [0, 50], [0, 50], [0, 50], [0, 2]])\n",
        "\n",
        "BO_m = BO(X_searchspace, budget=15000)\n",
        "BO_m.optimize()\n",
        "\n",
        "print(\"\\n--- OPTIMIZATION COMPLETE ---\")\n",
        "best_titre = -1 * min(BO_m.Y)\n",
        "print(f\"Highest Titre Found: {best_titre:.4f} g/L\")"
      ],
      "metadata": {
        "id": "HD52Q3AeKFrI",
        "outputId": "e8656651-45f0-4030-9468-7f13ce248915",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 740
        }
      },
      "id": "HD52Q3AeKFrI",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Warmup (Fidelity 0) ---\n",
            "   -> Ran Fidelity 0 (Cost: 10). Result: 6.8932 g/L. Budget Left: 14990\n",
            "   -> Ran Fidelity 0 (Cost: 10). Result: 6.8932 g/L. Budget Left: 14980\n",
            "   -> Ran Fidelity 0 (Cost: 10). Result: 6.8932 g/L. Budget Left: 14970\n",
            "   -> Ran Fidelity 0 (Cost: 10). Result: 6.8932 g/L. Budget Left: 14960\n",
            "   -> Ran Fidelity 0 (Cost: 10). Result: 6.8932 g/L. Budget Left: 14950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/botorch/models/utils/assorted.py:271: InputDataWarning: Data (input features) is not contained to the unit cube. Please consider min-max scaling the input data.\n",
            "  check_min_max_scaling(\n",
            "/usr/local/lib/python3.12/dist-packages/botorch/models/utils/assorted.py:274: InputDataWarning: Data (outcome observations) is not standardized (std = tensor([0.], dtype=torch.float64), mean = tensor([0.], dtype=torch.float64)).Please consider scaling the input to zero mean and unit variance.\n",
            "  check_standardization(Y=train_Y, raise_on_fail=raise_on_fail)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   -> Ran Fidelity 0 (Cost: 10). Result: 12.6396 g/L. Budget Left: 14940\n",
            "   -> Ran Fidelity 0 (Cost: 10). Result: 12.6271 g/L. Budget Left: 14930\n",
            "   -> Ran Fidelity 0 (Cost: 10). Result: 13.0513 g/L. Budget Left: 14920\n",
            "   -> Ran Fidelity 0 (Cost: 10). Result: 13.5138 g/L. Budget Left: 14910\n",
            "   -> Ran Fidelity 0 (Cost: 10). Result: 27.0981 g/L. Budget Left: 14900\n",
            "   -> Ran Fidelity 0 (Cost: 10). Result: 28.6695 g/L. Budget Left: 14890\n",
            "   -> Ran Fidelity 0 (Cost: 10). Result: 1.3773 g/L. Budget Left: 14880\n",
            "   -> Ran Fidelity 0 (Cost: 10). Result: 7.1641 g/L. Budget Left: 14870\n",
            "   -> Ran Fidelity 0 (Cost: 10). Result: 28.9049 g/L. Budget Left: 14860\n",
            "   -> Ran Fidelity 0 (Cost: 10). Result: 1.3749 g/L. Budget Left: 14850\n",
            "   -> Ran Fidelity 0 (Cost: 10). Result: 27.1344 g/L. Budget Left: 14840\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2052930833.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0mBO_m\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_searchspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbudget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m \u001b[0mBO_m\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n--- OPTIMIZATION COMPLETE ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2052930833.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;31m# 1. Fit GP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mgp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0mgp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;31m# 2. Get suggestion using EI / Cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2052930833.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mfit_gpytorch_mll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/botorch/fit.py\u001b[0m in \u001b[0;36mfit_gpytorch_mll\u001b[0;34m(mll, closure, optimizer, closure_kwargs, optimizer_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"optimizer\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m     return FitGPyTorchMLL(\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0mmll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlikelihood\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/botorch/utils/dispatcher.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mMDNotImplementedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;31m# Traverses registered methods in order, yields whenever a match is found\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/botorch/fit.py\u001b[0m in \u001b[0;36m_fit_fallback\u001b[0;34m(mll, _, __, closure, optimizer, closure_kwargs, optimizer_kwargs, max_attempts, pick_best_of_all_attempts, warning_handler, caught_exception_types, **ignore)\u001b[0m\n\u001b[1;32m    212\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mcatch_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwarning_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m                     \u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"always\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOptimizationWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptimizer_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                 \u001b[0;31m# Resolve warnings and determine whether or not to retry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/botorch/optim/fit.py\u001b[0m in \u001b[0;36mfit_gpytorch_mll_scipy\u001b[0;34m(mll, parameters, bounds, closure, closure_kwargs, method, options, callback, timeout_sec)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mclosure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mclosure_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     result = scipy_minimize(\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0mclosure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/botorch/optim/core.py\u001b[0m in \u001b[0;36mscipy_minimize\u001b[0;34m(closure, parameters, bounds, callback, x0, method, options, timeout_sec)\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pyre-ignore [29]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     raw = minimize_with_timeout(\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0mwrapped_closure\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mwrapped_closure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx0\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp_float64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/botorch/optim/utils/timeout.py\u001b[0m in \u001b[0;36mminimize_with_timeout\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options, timeout_sec)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;31m# See https://github.com/scipy/scipy/issues/22438.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mthreadpool_limits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlimits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_api\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"blas\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             return optimize.minimize(\n\u001b[0m\u001b[1;32m     87\u001b[0m                 \u001b[0mfun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0mx0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    782\u001b[0m                                  **options)\n\u001b[1;32m    783\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'l-bfgs-b'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m         res = _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0m\u001b[1;32m    785\u001b[0m                                callback=callback, **options)\n\u001b[1;32m    786\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tnc'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/optimize/_lbfgsb_py.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, workers, **unknown_options)\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m             \u001b[0;31m# Overwrite f and g:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m             \u001b[0;31m# new iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_and_grad\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_x\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36m_update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_update_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_updated\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0mfx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nfev\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lowest_f\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/_lib/_util.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;31m# Send a copy because the user may overwrite it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;31m# The user of this class might want `x` to remain unchanged.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mfx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnfev\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/optimize/_optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;34m\"\"\" returns the function value \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_if_needed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/optimize/_optimize.py\u001b[0m in \u001b[0;36m_compute_if_needed\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/botorch/optim/closures/core.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, state, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0mvalue_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp_float64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_gradient_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/botorch/optim/closures/core.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mzero_grad_ctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/botorch/optim/closures/model_closures.py\u001b[0m in \u001b[0;36mclosure\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;31m# The inputs will get transformed in forward here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0mmodel_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         log_likelihood = mll(\n\u001b[0m\u001b[1;32m    140\u001b[0m             \u001b[0mmodel_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gpytorch/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDistribution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLinearOperator\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_validate_module_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gpytorch/mlls/exact_marginal_log_likelihood.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, function_dist, target, *params, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;31m# Get the log prob of the marginal distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_other_terms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gpytorch/distributions/multivariate_normal.py\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    226\u001b[0m         \"\"\"\n\u001b[1;32m    227\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfast_computations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/distributions/multivariate_normal.py\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_batch_mahalanobis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unbroadcasted_scale_tril\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m         half_log_det = (\n\u001b[1;32m    257\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unbroadcasted_scale_tril\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiagonal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gpytorch/distributions/multivariate_normal.py\u001b[0m in \u001b[0;36m_unbroadcasted_scale_tril\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mislazy\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__unbroadcasted_scale_tril\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;31m# cache root decoposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0must\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy_covariance_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcholesky\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__unbroadcasted_scale_tril\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0must\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__unbroadcasted_scale_tril\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/linear_operator/operators/_linear_operator.py\u001b[0m in \u001b[0;36mcholesky\u001b[0;34m(self, upper)\u001b[0m\n\u001b[1;32m   1309\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCholesky\u001b[0m \u001b[0mfactor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlower\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mupper\u001b[0m \u001b[0mtriangular\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1310\u001b[0m         \"\"\"\n\u001b[0;32m-> 1311\u001b[0;31m         \u001b[0mchol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cholesky\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mupper\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m             \u001b[0mchol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transpose_nonbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/linear_operator/utils/memoize.py\u001b[0m in \u001b[0;36mg\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mkwargs_pkl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_in_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_pkl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs_pkl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_add_to_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_pkl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs_pkl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_get_from_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_pkl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs_pkl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/linear_operator/operators/_linear_operator.py\u001b[0m in \u001b[0;36m_cholesky\u001b[0;34m(self, upper)\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mlinear_operator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moperators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtriangular_linear_operator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTriangularLinearOperator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m         \u001b[0mevaluated_kern_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_mat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKeOpsLinearOperator\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msub_mat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mevaluated_kern_mat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/linear_operator/operators/added_diag_linear_operator.py\u001b[0m in \u001b[0;36mevaluate_kernel\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0madded_diag_linear_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepresentation_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepresentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0madded_diag_linear_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_linear_op\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0madded_diag_linear_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_diag_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/linear_operator/operators/_linear_operator.py\u001b[0m in \u001b[0;36mrepresentation_tree\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2070\u001b[0m         \u001b[0mincluding\u001b[0m \u001b[0mall\u001b[0m \u001b[0msubobjects\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mThis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mused\u001b[0m \u001b[0minternally\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2071\u001b[0m         \"\"\"\n\u001b[0;32m-> 2072\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mLinearOperatorRepresentationTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2074\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/linear_operator/operators/linear_operator_representation_tree.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, linear_op)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinear_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_differentiable_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"representation\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepresentation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Is it a lazy tensor?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0mrepresentation_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepresentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounter\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrepresentation_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepresentation_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0mcounter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mrepresentation_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\u001b[0m in \u001b[0;36mrepresentation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;31m# representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepresentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrepresentation_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gpytorch/utils/memoize.py\u001b[0m in \u001b[0;36mg\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mkwargs_pkl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_in_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_pkl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs_pkl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_add_to_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_pkl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs_pkl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_get_from_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_pkl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs_pkl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_grad_enabled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\u001b[0m in \u001b[0;36mevaluate_kernel\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mtemp_active_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactive_dims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactive_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             res = self.kernel(\n\u001b[0m\u001b[1;32m    356\u001b[0m                 \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                 \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gpytorch/kernels/kernel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x1, x2, diag, last_dim_is_batch, **params)\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m                 res = to_linear_operator(\n\u001b[0;32m--> 534\u001b[0;31m                     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_dim_is_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlast_dim_is_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m                 )\n\u001b[1;32m    536\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gpytorch/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDistribution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLinearOperator\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_validate_module_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gpytorch/kernels/rbf_kernel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x1, x2, diag, **params)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mx1_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlengthscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mx2_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlengthscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mpostprocess_rbf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcovar_dist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msquare_dist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         return RBFCovariance.apply(\n\u001b[1;32m     79\u001b[0m             \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gpytorch/kernels/kernel.py\u001b[0m in \u001b[0;36mcovar_dist\u001b[0;34m(self, x1, x2, diag, last_dim_is_batch, square_dist, **params)\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0mdist_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msq_dist\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msquare_dist\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdist_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1_eq_x2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexpand_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0msizes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mKernel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gpytorch/kernels/kernel.py\u001b[0m in \u001b[0;36msq_dist\u001b[0;34m(x1, x2, x1_eq_x2)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0madjustment\u001b[0m  \u001b[0;31m# x1 and x2 should be identical in all dims except -2 at this point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mx2_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mx2_pad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mx1_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1_pad\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "print(\"--- DIAGNOSTIC TEST ---\")\n",
        "\n",
        "# 1. Create two manually different recipes (Lists)\n",
        "recipe_A = [30.5, 6.2, 10.0, 10.0, 10.0, 0] # Low Temp\n",
        "recipe_B = [39.5, 7.8, 40.0, 40.0, 40.0, 0] # High Temp\n",
        "\n",
        "print(f\"Testing Recipe A: {recipe_A}\")\n",
        "try:\n",
        "    # Try passing as a list\n",
        "    result_A = conduct_experiment([recipe_A])[0]\n",
        "    print(f\" -> Result A (List): {result_A}\")\n",
        "except Exception as e:\n",
        "    print(f\" -> List Method Failed: {e}\")\n",
        "\n",
        "print(f\"\\nTesting Recipe B: {recipe_B}\")\n",
        "try:\n",
        "    # Try passing as a numpy array (Common fix)\n",
        "    result_B = conduct_experiment(np.array([recipe_B]))[0]\n",
        "    print(f\" -> Result B (Numpy): {result_B}\")\n",
        "except Exception as e:\n",
        "    print(f\" -> Numpy Method Failed: {e}\")\n",
        "\n",
        "print(\"\\n--- CONCLUSION ---\")\n",
        "if result_A == result_B:\n",
        "    print(\"CRITICAL ISSUE: The function returns the same value for different inputs!\")\n",
        "    print(\"Possibility 1: Fidelity 0 is broken in the hackathon environment.\")\n",
        "    print(\"Possibility 2: We are formatting the input wrong.\")\n",
        "else:\n",
        "    print(\"SUCCESS: The function works! The inputs produce different outputs.\")"
      ],
      "metadata": {
        "id": "y0e_-z74MDu5",
        "outputId": "646c340d-a353-4865-b564-0885e786c91a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "y0e_-z74MDu5",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- DIAGNOSTIC TEST ---\n",
            "Testing Recipe A: [30.5, 6.2, 10.0, 10.0, 10.0, 0]\n",
            " -> Result A (List): 7.060440464451218\n",
            "\n",
            "Testing Recipe B: [39.5, 7.8, 40.0, 40.0, 40.0, 0]\n",
            " -> Result B (Numpy): 1.3630162220337492\n",
            "\n",
            "--- CONCLUSION ---\n",
            "SUCCESS: The function works! The inputs produce different outputs.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "test_botorch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}