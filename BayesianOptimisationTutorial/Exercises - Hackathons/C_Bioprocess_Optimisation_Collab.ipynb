{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6023a9d2-d1b5-4383-9701-7131b0a6ed0b",
      "metadata": {
        "id": "6023a9d2-d1b5-4383-9701-7131b0a6ed0b"
      },
      "source": [
        "### Optimisation of a Bioprocess with Multifidelity Bayesian Optimisation\n",
        "\n",
        "\n",
        "#### Hackathon Breif\n",
        "This hackathon involves the optimisation of a simulated bioprocess at process scale utilising CHO cells to produce a desired protein. Experimentally, this would involve a resource-intensive screening campaign involving the growth and feeding of cells under precise conditions (temperature, pH, feed amount, fidelity, etc.) to maximize the production of a desired product. This hackathon offers a simulated method of mapping bioprocess input parameters to a final predicted titre concentration: a measure of cell productivity. The simulations are based on various kinetic parameters which are unique to the type of cells used. For the final scoring, a different set of cell kinetic parameters will be used to evaluate your algorithm.\n",
        "\n",
        "#### Inputs and Outputs\n",
        "Inputs to the bioprocess includes 5 vairables: the temperature [°C], pH and the concentration of feed [mM] at 3 different timepoints over 150 minutes. The output is the concentration of the titre (desired product) [g/L]. The goal is to obtain the input variables that correspond to the highest obtained titre.\n",
        "\n",
        "The bounds of the inputs are as follows:\n",
        "\n",
        "```\n",
        "temperature [°C]               -> 30 - 40\n",
        "pH                             -> 6 - 8\n",
        "first feed concentration [mM]  -> 0 - 50\n",
        "second feed concentration [mM] -> 0 - 50\n",
        "third feed concentration [mM]  -> 0 - 50\n",
        "```\n",
        "\n",
        "#### Fidelities and Running the simulation\n",
        "The simulations can be perfomed at 3 levels of fidelities with an associated accuracy and costs. These fidelities corresponds to a different reactor type and scale used.\n",
        "\n",
        "```\n",
        "Lowest fideility: 3L reactor with 1 feeding timepoint at 60 mins.\n",
        "Realtive cost: 10\n",
        "Remarks: The feeding concentration is taken as the second feed concentration. Lowest accuracy, but also lowest cost.\n",
        "\n",
        "Middle fidelity: 3L reactor with 3 feeding timepoints at 40, 80, 120 mins.\n",
        "Relative cost: 575\n",
        "Remarks: -\n",
        "\n",
        "Highest fidelity: 15L reactor with 3 feeding timepoints at 40, 80, 120 mins.\n",
        "Relative cost: 2100\n",
        "Remarks: Highest accuracy but high cost.\n",
        "```\n",
        "\n",
        "To run an experiment, one can use the `conduct_experiment(X)` function -> this is your objective function. The inputs to this function is a matrix of shape (N, 6) where N is the number of data points and 6 refers to the total number of variables in the following order: `[temperature, pH, feed1, feed2, feed3, fidelity]`. The fidelities are refered to as integers where `0` corresponds to the lowest fidelity, `1` with the middle and `2` with the highest fidelity. An example is shown below.\n",
        "\n",
        "``` python\n",
        "import numpy as np\n",
        "def obj_func(X):\n",
        "\treturn (-np.array(conduct_experiment(X))) #negative placed if optimisation performed is minimisation\n",
        "\n",
        "X_initial = np.array([[33, 6.25, 10, 20, 20, 0],\n",
        "                      [38, 8, 20, 10, 20, 0]])\n",
        "Y_initial = conduct_experiment(X_initial)\n",
        "print(Y_initial)\n",
        "```\n",
        "\n",
        "#### Goal and Submission\n",
        "Your goal is to develop a Bayesian Optimisation class to obtain the set of inputs which **maximizes the titre at the highest fideility**. You have a **budget of 15000** (observe the cost of running each fidelity) and starting with a maximum of 7 training points that is not a part of the budget. (Remember, you have to have at least 2 points for each variable for the covariance matrix to be calculated.)\n",
        "\n",
        "Please submit your BO class (and GP class) along with the execution block as a .py file to the instructor. A different cell type (with different simulation parameters and maxima) will be used for scoring.\n",
        "\n",
        "This hackathon will be scored based on maximum titre concentration obtained at the highest fidelity. You must stay within the allocated budget! This will be checked, and if exceeded, your submission will be disqualified!\n",
        "\n",
        "#### Form of the BO class and execution block\n",
        "You are allowed to write your own BO class or make modifications to any of the previously seen BO classes.\n",
        "\n",
        "You must include the attributes `self.X` and `self.Y` corresponding to all of your evaluated inputs and outputs as this will be used to retrive the information used for scoring.\n",
        "\n",
        "```python\n",
        "#submission should look something like the following\n",
        "class GP: #if you have any separate classes other than the BO class\n",
        "    def __init__(self, ...):\n",
        "        ...\n",
        "#BO class\n",
        "class BO:\n",
        "    def __init__(self, ...):\n",
        "        self.X = #training data which the evaluated data is to be appended\n",
        "        self.Y = #evaluated via the objective function using self.X\n",
        "\n",
        "# BO Execution Block\n",
        "X_training = [...]\n",
        "X_seachspace = [...]\n",
        "\n",
        "BO_m = BO(...)\n",
        "```\n",
        "\n",
        "#### Guidance (Intermediate - Multi-batch Bayesian Optimsation)\n",
        "You can construct a single-sequential or batch BO algorithm to perform the optimisation. The lowest fidelity experiments do not offer accurate outcomes and you have to choose how many number of expeirments for each fidelity to be performed such that you do not exceed your allocated budget. To link between each fideility, one could perform optimisation on the lower fidilities and then translate the best input conditions to run the highest fidelity experiment.\n",
        "\n",
        "#### Guidance (Advanced - Multi-fidelity Bayesian Optimisation)\n",
        "You can develop a multi-fidelity Bayesian Optimisation algorithm to perform the optimisation. Since the score is based on the highest titre concentration of the highest fidelity, it might be beneficial if you constrain (at least) the last experiment to be run with the highest fidelity - this mitigates the risk that your algorithm does not perform any experiments with the highest fidelity. A basic MFBO algorithm could be created by modifying the acquisition function to one that is cost aware. For example: we have previously used Lower Confidence Bound to balance exploration and exploitation of the search space (see notebook section C). To make this cost aware, we can scale the values obtained from LCB by the cost.\n",
        "\n",
        "```python\n",
        "    def MF_lower_confidence_bound(...):\n",
        "        lower_std = Ysearchspace_mean - acquisition_hyperparam[0]*np.sqrt(Ysearchspace_std)\n",
        "        # mf_lower_std = lower_std / assocated cost for each simulation\n",
        "        return (X_searchspace[np.argmin(mf_lower_std)])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d57ac5a5",
      "metadata": {
        "id": "d57ac5a5"
      },
      "source": [
        "#### Feedback and Scoring Example\n",
        "Once your algorithm is submitted to the instructor, you can request for some feedback on the performance of your algorithm. The final score will be calculated based on the maximum titre concentration obtained from the highest fidelity. 3 plots will be produced to showcase the performance of your algorithm and the performance against the cohort. Example:\n",
        "\n",
        "![image.png](attachment:image.png)\n",
        "![image-2.png](attachment:image-2.png)\n",
        "![image-3.png](attachment:image-3.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bd73484-4185-4126-8d42-fd3b5258cc61",
      "metadata": {
        "id": "4bd73484-4185-4126-8d42-fd3b5258cc61"
      },
      "source": [
        "#### Package Imports\n",
        "\n",
        "Packages are limited to the the ones listed in the package cell - Talk to one of the intructors to ask if it is possible to import other packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "efc60248-a788-4874-9922-36c336a69fcd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efc60248-a788-4874-9922-36c336a69fcd",
        "outputId": "542c73c5-ba3c-4017-c034-bd59da2eb255"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sobol_seq\n",
            "  Downloading sobol_seq-0.2.0-py3-none-any.whl.metadata (273 bytes)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sobol_seq) (1.16.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from sobol_seq) (2.0.2)\n",
            "Downloading sobol_seq-0.2.0-py3-none-any.whl (9.2 kB)\n",
            "Installing collected packages: sobol_seq\n",
            "Successfully installed sobol_seq-0.2.0\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (5.24.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly) (9.1.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from plotly) (26.0)\n",
            "Collecting gpytorch\n",
            "  Downloading gpytorch-1.15.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting jaxtyping (from gpytorch)\n",
            "  Downloading jaxtyping-0.3.7-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: mpmath<=1.3,>=0.19 in /usr/local/lib/python3.12/dist-packages (from gpytorch) (1.3.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from gpytorch) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from gpytorch) (1.16.3)\n",
            "Collecting linear_operator>=0.6 (from gpytorch)\n",
            "  Downloading linear_operator-0.6-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.12/dist-packages (from linear_operator>=0.6->gpytorch) (2.9.0+cpu)\n",
            "Requirement already satisfied: numpy<2.6,>=1.25.2 in /usr/local/lib/python3.12/dist-packages (from scipy>=1.6.0->gpytorch) (2.0.2)\n",
            "Collecting wadler-lindig>=0.1.3 (from jaxtyping->gpytorch)\n",
            "  Downloading wadler_lindig-0.1.7-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->gpytorch) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->gpytorch) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->linear_operator>=0.6->gpytorch) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->linear_operator>=0.6->gpytorch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->linear_operator>=0.6->gpytorch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->linear_operator>=0.6->gpytorch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->linear_operator>=0.6->gpytorch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->linear_operator>=0.6->gpytorch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->linear_operator>=0.6->gpytorch) (2025.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0->linear_operator>=0.6->gpytorch) (3.0.3)\n",
            "Downloading gpytorch-1.15.1-py3-none-any.whl (287 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.8/287.8 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading linear_operator-0.6-py3-none-any.whl (176 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.3/176.3 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxtyping-0.3.7-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.1/56.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wadler_lindig-0.1.7-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: wadler-lindig, jaxtyping, linear_operator, gpytorch\n",
            "Successfully installed gpytorch-1.15.1 jaxtyping-0.3.7 linear_operator-0.6 wadler-lindig-0.1.7\n",
            "Collecting rdkit\n",
            "  Downloading rdkit-2025.9.4-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rdkit) (2.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from rdkit) (11.3.0)\n",
            "Downloading rdkit-2025.9.4-cp312-cp312-manylinux_2_28_x86_64.whl (36.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.6/36.6 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rdkit\n",
            "Successfully installed rdkit-2025.9.4\n",
            "Collecting botorch\n",
            "  Downloading botorch-0.16.1-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from botorch) (4.15.0)\n",
            "Collecting pyre_extensions (from botorch)\n",
            "  Downloading pyre_extensions-0.0.32-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: gpytorch>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from botorch) (1.15.1)\n",
            "Requirement already satisfied: linear_operator>=0.6 in /usr/local/lib/python3.12/dist-packages (from botorch) (0.6)\n",
            "Requirement already satisfied: torch>=2.0.1 in /usr/local/lib/python3.12/dist-packages (from botorch) (2.9.0+cpu)\n",
            "Collecting pyro-ppl>=1.8.4 (from botorch)\n",
            "  Downloading pyro_ppl-1.9.1-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from botorch) (1.16.3)\n",
            "Requirement already satisfied: multipledispatch in /usr/local/lib/python3.12/dist-packages (from botorch) (1.0.0)\n",
            "Requirement already satisfied: threadpoolctl in /usr/local/lib/python3.12/dist-packages (from botorch) (3.6.0)\n",
            "Requirement already satisfied: jaxtyping in /usr/local/lib/python3.12/dist-packages (from gpytorch>=1.14.2->botorch) (0.3.7)\n",
            "Requirement already satisfied: mpmath<=1.3,>=0.19 in /usr/local/lib/python3.12/dist-packages (from gpytorch>=1.14.2->botorch) (1.3.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from gpytorch>=1.14.2->botorch) (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.12/dist-packages (from pyro-ppl>=1.8.4->botorch) (2.0.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from pyro-ppl>=1.8.4->botorch) (3.4.0)\n",
            "Collecting pyro-api>=0.1.1 (from pyro-ppl>=1.8.4->botorch)\n",
            "  Downloading pyro_api-0.1.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.12/dist-packages (from pyro-ppl>=1.8.4->botorch) (4.67.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.1->botorch) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.1->botorch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.1->botorch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.1->botorch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.1->botorch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.1->botorch) (2025.3.0)\n",
            "Collecting typing-inspect (from pyre_extensions->botorch)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: wadler-lindig>=0.1.3 in /usr/local/lib/python3.12/dist-packages (from jaxtyping->gpytorch>=1.14.2->botorch) (0.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.1->botorch) (3.0.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->gpytorch>=1.14.2->botorch) (1.5.3)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect->pyre_extensions->botorch)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading botorch-0.16.1-py3-none-any.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyro_ppl-1.9.1-py3-none-any.whl (755 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyre_extensions-0.0.32-py3-none-any.whl (12 kB)\n",
            "Downloading pyro_api-0.1.2-py3-none-any.whl (11 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: pyro-api, mypy-extensions, typing-inspect, pyro-ppl, pyre_extensions, botorch\n",
            "Successfully installed botorch-0.16.1 mypy-extensions-1.1.0 pyre_extensions-0.0.32 pyro-api-0.1.2 pyro-ppl-1.9.1 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "# if using google collab, run the following pip installs!\n",
        "!pip install sobol_seq\n",
        "!pip install plotly\n",
        "!pip install gpytorch\n",
        "!pip install rdkit\n",
        "!pip install botorch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "977ab0af-7637-40a7-ab13-e7c3491d0477",
      "metadata": {
        "id": "977ab0af-7637-40a7-ab13-e7c3491d0477"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import numpy.random as rnd\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import axes3d, Axes3D\n",
        "import plotly.graph_objs as go\n",
        "from scipy.integrate import quad\n",
        "from scipy.spatial.distance import cdist\n",
        "from scipy.optimize import minimize, differential_evolution, NonlinearConstraint\n",
        "from sklearn.decomposition import PCA\n",
        "import math\n",
        "import time\n",
        "import sobol_seq\n",
        "import torch\n",
        "import gpytorch\n",
        "import copy\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d38720ec",
      "metadata": {
        "id": "d38720ec"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "reactor_list = [\"3LBATCH\", \"3LCONTBATCH\", \"15LCONTBATCH\"]\n",
        "process_parameters = {\n",
        "    \"3LBATCH\": {\n",
        "        \"celltype_1\": {\"my_max\": 0.035, \"K_lysis\": 4e-2,   \"k\": [1e-3, 1e-2, 1e-2],          \"K\": [150, 40, 1, 0.22],    \"Y\": [9.23e7, 8.8e8, 1.6, 0.68, 6.2292e-8, 4.41e-6],    \"m\": [8e-13, 3e-12], \"A\": 1e1, \"pH_opt\": 7.2, \"E_a\": 32},\n",
        "\n",
        "    },\n",
        "    \"3LCONTBATCH\": {\n",
        "        \"celltype_1\": {\"my_max\": 0.035, \"K_lysis\": 4e-2,   \"k\": [1e-3, 1e-2, 1e-2],          \"K\": [150, 40, 1, 0.22],    \"Y\": [9.23e7, 8.8e8, 1.6, 0.68, 6.2292e-8, 4.41e-6],    \"m\": [8e-13, 3e-12], \"A\": 1e1, \"pH_opt\": 7.2, \"E_a\": 32},\n",
        "\n",
        "    },\n",
        "    \"15LCONTBATCH\": {\n",
        "        \"celltype_1\": {\"my_max\": 0.035, \"K_lysis\": 4e-2,   \"k\": [1e-3, 1e-2, 1e-2],          \"K\": [150, 40, 1, 0.22],    \"Y\": [9.23e7, 8.8e8, 1.6, 0.68, 6.2292e-8, 4.41e-6],    \"m\": [8e-13, 3e-12], \"A\": 1e1, \"pH_opt\": 7.2, \"E_a\": 32},\n",
        "\n",
        "    }\n",
        "}\n",
        "NOISE_LEVEL = {\n",
        "            \"3LBATCH\": 2e-1,\n",
        "            \"3LCONTBATCH\": 8e-2,\n",
        "            \"15LCONTBATCH\": 8e-5\n",
        "        }\n",
        "fidelity_cost = {\n",
        "            \"3LBATCH\": 0.05,\n",
        "            \"3LCONTBATCH\": 0.5,\n",
        "            \"15LCONTBATCH\": 1\n",
        "        }\n",
        "data = []\n",
        "for reactor, cell_data in process_parameters.items():\n",
        "    for cell_type, params in cell_data.items():\n",
        "        entry = {\n",
        "            \"reactor\": reactor,\n",
        "            \"cell_type\": cell_type,\n",
        "            **params\n",
        "        }\n",
        "        data.append(entry)\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from scipy.integrate import solve_ivp\n",
        "# import C_Bioprocess_Utils.conditions_data as data\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "class EXPERIMENT:\n",
        "    def __init__(\n",
        "            self,\n",
        "            T: float = 32,\n",
        "            pH: float = 7.2,\n",
        "            cell_type: str = \"celltype_1\",\n",
        "            reactor: str = \"3LBATCH\",\n",
        "            feeding: list = [(10, 0), (20, 0), (30, 0)],\n",
        "            time=150,\n",
        "            df =df\n",
        "        ):\n",
        "\n",
        "        df = df\n",
        "        params = df[(df['reactor'] == reactor) & (df['cell_type'] == cell_type)]\n",
        "        self.reactor    = reactor\n",
        "        self.volume     = 3\n",
        "        self.cell_type  = cell_type\n",
        "        self.time       = time\n",
        "        self.my_max     = params[\"my_max\"].iloc[0]\n",
        "        self.K_lysis    = params[\"K_lysis\"].iloc[0]\n",
        "        self.K_L, self.K_A, self.K_G, self.K_Q = params[\"K\"].iloc[0]\n",
        "        self.Y = params[\"Y\"].iloc[0]\n",
        "        self.m = params[\"m\"].iloc[0]\n",
        "        self.k_d_Q, self.k_d_max, self.k_my = params[\"k\"].iloc[0]\n",
        "\n",
        "        self.A      = params[\"A\"].iloc[0]\n",
        "        self.E_a    = params[\"E_a\"].iloc[0]\n",
        "        self.pH_opt = params[\"pH_opt\"].iloc[0]\n",
        "\n",
        "        self.initial_conditions = [0, 1e6, 0.8 * 1e6, 0, 210, 1, 9, 0]\n",
        "        self.solution = None\n",
        "        self.t = None\n",
        "        self.T         = T\n",
        "        self.pH        = pH\n",
        "\n",
        "        self.feeding = feeding\n",
        "\n",
        "        self.R = 8.314\n",
        "\n",
        "    def temperature_effect(self):\n",
        "        x = self.T\n",
        "        mu = self.E_a\n",
        "        A = 5\n",
        "\n",
        "        left_part = np.exp(-1 * ((x - mu) / 10)**2)\n",
        "        right_part = np.exp(-0.9 * ((x - mu) / 3.6)**2)\n",
        "\n",
        "        factor = A * np.where(x < mu, left_part, right_part)\n",
        "        return factor\n",
        "\n",
        "\n",
        "    def pH_effect(self) -> float:\n",
        "        x = self.pH\n",
        "        mu = self.pH_opt\n",
        "        A = 2\n",
        "\n",
        "        left_part = np.exp(-0.8 * ((x - mu) / 1)**2)\n",
        "        right_part = np.exp(-1 * ((x - mu) / 0.5)**2)\n",
        "\n",
        "        factor = A * np.where(x < mu, left_part, right_part)\n",
        "        return factor\n",
        "\n",
        "    def my(self, G, Q, L, A):\n",
        "        temperature_factor = self.temperature_effect()\n",
        "        pH_factor = self.pH_effect()\n",
        "\n",
        "        my_max = self.my_max\n",
        "        K_G = self.K_G\n",
        "        K_Q = self.K_Q\n",
        "        K_L = self.K_L\n",
        "        K_A = self.K_A\n",
        "\n",
        "        my = my_max * G/(K_G + G) * Q/(K_Q + Q) * K_L/(K_L + L) * K_A/(K_A + A) * temperature_factor * pH_factor\n",
        "        return my\n",
        "\n",
        "    def ODE(self,t,x):\n",
        "        P, X_T, X_V, X_D, G, Q, L, A = x\n",
        "        my = self.my(G, Q, L, A)\n",
        "        k_d = self.k_d_max * (self.k_my/(my + self.k_my))\n",
        "        K_lysis = self.K_lysis\n",
        "        k_d_Q = self.k_d_Q\n",
        "        K_G = self.K_G\n",
        "\n",
        "        Y_X_G, Y_X_Q, Y_L_G, Y_A_Q, Y_P_X, Y_dot_P_X = self.Y\n",
        "        m_G, m_Q = self.m\n",
        "\n",
        "        dX_T_dt = my * X_V - K_lysis * X_D\n",
        "        dX_V_dt = (my-k_d) * X_V\n",
        "        dX_D_dt = k_d * X_V - K_lysis * X_D\n",
        "\n",
        "        dP_dt = Y_P_X * X_T + Y_dot_P_X * (my * G / (K_G + G)) * X_V\n",
        "\n",
        "        dG_dt = X_V * (-my/Y_X_G - m_G)\n",
        "        dQ_dt = X_V * (-my/Y_X_Q - m_Q) - k_d_Q * Q\n",
        "        dL_dt = -X_V * Y_L_G * (-my/Y_X_G - m_G)\n",
        "        dA_dt = -X_V * Y_A_Q * (-my/Y_X_Q - m_Q) + k_d_Q * Q\n",
        "\n",
        "        gradients = [dP_dt, dX_T_dt, dX_V_dt, dX_D_dt, dG_dt, dQ_dt, dL_dt, dA_dt]\n",
        "\n",
        "        return gradients\n",
        "\n",
        "    def ODE_solver(self):\n",
        "        t_span = (0, self.time)\n",
        "        t_eval_total = []\n",
        "        y_total = []\n",
        "        current_t = 0\n",
        "        current_y = self.initial_conditions.copy()\n",
        "\n",
        "        for event_time, new_G_value in self.feeding:\n",
        "            t_span_segment = (current_t, event_time)\n",
        "            t_eval_segment = np.linspace(current_t, event_time, 1000)\n",
        "\n",
        "            solution = solve_ivp(\n",
        "                fun=self.ODE,\n",
        "                t_span=t_span_segment,\n",
        "                y0=current_y,\n",
        "                t_eval=t_eval_segment,\n",
        "                method=\"RK45\"\n",
        "            )\n",
        "\n",
        "            t_eval_total.extend(solution.t)\n",
        "            y_total.append(solution.y)\n",
        "\n",
        "            current_t = event_time\n",
        "            current_y = solution.y[:, -1]\n",
        "            if current_y[4] < new_G_value:\n",
        "                current_y[4] = new_G_value\n",
        "\n",
        "            new_Q_value = new_G_value * 0.4\n",
        "            if current_y[5] < new_Q_value:\n",
        "                current_y[5] = new_Q_value\n",
        "\n",
        "        t_span_segment = (current_t, self.time)\n",
        "        t_eval_segment = np.linspace(current_t, self.time, 500)\n",
        "\n",
        "        solution = solve_ivp(\n",
        "            fun=self.ODE,\n",
        "            t_span=t_span_segment,\n",
        "            y0=current_y,\n",
        "            t_eval=t_eval_segment,\n",
        "            method=\"RK45\"\n",
        "        )\n",
        "\n",
        "        t_eval_total.extend(solution.t)\n",
        "        y_total.append(solution.y)\n",
        "\n",
        "        t_eval_total = np.array(t_eval_total)\n",
        "        y_total = np.hstack(y_total)\n",
        "\n",
        "        self.solution = y_total\n",
        "        self.solution[0] = self.solution[0] / (self.volume * 1e3) # Transform unit into g/L\n",
        "\n",
        "        self.t = t_eval_total\n",
        "        return y_total\n",
        "\n",
        "    def measurement(self, noise_level=None, quantity=\"P\"):\n",
        "        # NOTE: this makes every call produce the same noise\n",
        "        # If you want different noise each call, remove this line.\n",
        "        np.random.seed(1234)\n",
        "\n",
        "        reactor_type = self.reactor\n",
        "\n",
        "        # noise_level can be:\n",
        "        #  - None (use default map)\n",
        "        #  - dict (map per reactor)\n",
        "        #  - number (use directly)\n",
        "        if noise_level is None:\n",
        "            noise_level = NOISE_LEVEL.get(reactor_type, None)\n",
        "            if noise_level is None:\n",
        "                raise ValueError(f\"Unknown reactor type: {reactor_type}\")\n",
        "        elif isinstance(noise_level, dict):\n",
        "            noise_level = noise_level.get(reactor_type, None)\n",
        "            if noise_level is None:\n",
        "                raise ValueError(f\"Unknown reactor type in provided noise map: {reactor_type}\")\n",
        "        else:\n",
        "            noise_level = float(noise_level)\n",
        "\n",
        "        self.ODE_solver()\n",
        "\n",
        "        index = {\"P\": 0, \"X_T\": 1, \"X_V\": 2, \"X_D\": 3, \"G\": 4, \"Q\": 5, \"L\": 6, \"A\": 7}\n",
        "        true_value = self.solution[index[quantity]][-1]\n",
        "\n",
        "        noise_magnitude = max(noise_level * true_value, 1e-8)\n",
        "        noise = np.random.normal(0, noise_magnitude)\n",
        "        return true_value + noise\n",
        "\n",
        "\n",
        "def conduct_experiment(X, initial_conditions: list = [0, 0.4 * 1e9, 0.4 * 1e6, 0, 20, 3.5, 0, 1.8], noise_level=None):\n",
        "    result = []\n",
        "    feeding = [(10, 0), (20, 0), (30, 0)]\n",
        "    reactor = \"3LBATCH\"\n",
        "\n",
        "    for row in X:\n",
        "        if len(row) == 2:\n",
        "            T, pH = row\n",
        "        elif len(row) == 5:\n",
        "            T, pH, F1, F2, F3 = row\n",
        "            feeding = [(40, float(F1)), (80, float(F2)), (120, float(F3))]\n",
        "        elif len(row) == 6:\n",
        "            T, pH, F1, F2, F3, fidelity = row\n",
        "            if np.round(fidelity) == 0:\n",
        "                feeding = [(40, 0), (60, float(F2)), (120, 0)]\n",
        "            else:\n",
        "                feeding = [(40, float(F1)), (80, float(F2)), (120, float(F3))]\n",
        "            reactor = reactor_list[int(np.round(fidelity))]\n",
        "        else:\n",
        "            raise ValueError(f\"Cannot handle the dimensionality of X. n must be 2, 5 or 6 but is {len(row)}\")\n",
        "\n",
        "        cell = EXPERIMENT(T=T, pH=pH, time=150, feeding=feeding, reactor=reactor)\n",
        "        cell.initial_conditions = initial_conditions\n",
        "        value = float(cell.measurement(quantity=\"P\", noise_level=noise_level))\n",
        "        #print(value)\n",
        "        result.append(value)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eae15efe-9687-418d-9b4d-32031d09ae78",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eae15efe-9687-418d-9b4d-32031d09ae78",
        "outputId": "48bc164d-d465-4c92-92cd-356b7b8b5b3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10.355266633865073, 1.3630867118273515]\n"
          ]
        }
      ],
      "source": [
        "# Check if this runs without errors!\n",
        "\n",
        "def obj_func(X):\n",
        "\treturn (-np.array(conduct_experiment(X))) #negative placed if optimisation performed is minimisation\n",
        "\n",
        "X_initial = np.array([[33, 6.25, 10, 20, 20, 0],\n",
        "                      [38, 8, 20, 10, 20, 0]])\n",
        "Y_initial = conduct_experiment(X_initial)\n",
        "print(Y_initial)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "751ac447",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "751ac447",
        "outputId": "339aa704-d4a0-4b53-90a6-92d9808916f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cheap Result: [10.240281638386119] (Cost: 10)\n",
            "Expensive Result: [13.328600225623239] (Cost: 2100)\n"
          ]
        }
      ],
      "source": [
        "# Let's try a LOW fidelity run (Last number is 0)\n",
        "# Temp=33, pH=7.0, F1=10, F2=10, F3=10, Fid=0\n",
        "X_cheap = np.array([[33, 7.0, 10, 10, 10, 0]])\n",
        "Y_cheap = conduct_experiment(X_cheap)\n",
        "print(f\"Cheap Result: {Y_cheap} (Cost: 10)\")\n",
        "\n",
        "# Let's try a HIGH fidelity run (Last number is 2)\n",
        "# Same inputs, just better machine\n",
        "X_expensive = np.array([[33, 7.0, 10, 10, 10, 2]])\n",
        "Y_expensive = conduct_experiment(X_expensive)\n",
        "print(f\"Expensive Result: {Y_expensive} (Cost: 2100)\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BO:\n",
        "    def __init__(self, bounds, budget=15000):\n",
        "        # 1. Setup the \"Memory\"\n",
        "        self.X = [] # Stores inputs: [Temp, pH, F1, F2, F3, Fidelity]\n",
        "        self.Y = [] # Stores outputs: [Titre]\n",
        "\n",
        "        # 2. Setup the \"Wallet\"\n",
        "        self.budget = budget\n",
        "        self.bounds = bounds\n",
        "        self.iteration = 0  # Starts at 0\n",
        "\n",
        "        # 3. Cost Menu (from the problem description)\n",
        "        self.cost_map = {0: 10, 1: 575, 2: 2100}\n",
        "\n",
        "    def update_data(self, x_new, y_new):\n",
        "        \"\"\"Adds new experiment data and deducts the cost.\"\"\"\n",
        "        self.X.append(x_new)\n",
        "        self.Y.append(y_new)\n",
        "\n",
        "        # Identify fidelity (the last column of x_new)\n",
        "        fidelity = int(x_new[-1])\n",
        "        cost = self.cost_map[fidelity]\n",
        "\n",
        "        self.budget -= cost\n",
        "\n",
        "        # Update Clock\n",
        "        self.iteration += 1 # Counts up by 1\n",
        "\n",
        "        # Format the recipe for easy reading\n",
        "        recipe = f\"T={x_new[0]:.1f}, pH={x_new[1]:.1f}, Feeds=[{x_new[2]:.0f}, {x_new[3]:.0f}, {x_new[4]:.0f}]\"\n",
        "\n",
        "        # Print the full status\n",
        "        print(f\"ITERATION {self.iteration}:\")\n",
        "        print(f\"   -> Machine Used: Fidelity {fidelity} (Cost: {cost})\")\n",
        "        print(f\"   -> Recipe Tried: {recipe}\")\n",
        "        print(f\"   -> RESULT (Y):   {y_new[0]:.4f} g/L\") # [cite: 7]\n",
        "        print(f\"   -> Budget Left:  {self.budget}\\n\")\n",
        "\n",
        "    def suggest_next_point(self):\n",
        "        \"\"\"\n",
        "        PLACEHOLDER: Currently acts as a 'Random Search'.\n",
        "        Later, you will replace this with your Gaussian Process logic.\n",
        "        \"\"\"\n",
        "        # Generate random inputs within bounds\n",
        "        # Temp (30-40), pH (6-8), Feeds (0-50)\n",
        "        next_x = [\n",
        "            np.random.uniform(self.bounds[0,0], self.bounds[0,1]),\n",
        "            np.random.uniform(self.bounds[1,0], self.bounds[1,1]),\n",
        "            np.random.uniform(self.bounds[2,0], self.bounds[2,1]),\n",
        "            np.random.uniform(self.bounds[3,0], self.bounds[3,1]),\n",
        "            np.random.uniform(self.bounds[4,0], self.bounds[4,1]),\n",
        "            # Randomly choose a fidelity (0, 1, or 2) for now\n",
        "            np.random.randint(0, 3)\n",
        "        ]\n",
        "        return np.array(next_x)\n",
        "\n",
        "        # 1. Initialize\n",
        "# Bounds: Temp(30-40), pH(6-8), F1(0-50), F2(0-50), F3(0-50), Fid(0-2)\n",
        "bounds = np.array([\n",
        "    [30, 40], # Temperature\n",
        "    [6, 8],   # pH\n",
        "    [0, 50],  # Feed 1\n",
        "    [0, 50],  # Feed 2\n",
        "    [0, 50],  # Feed 3\n",
        "    [0, 2]    # Fidelity (0=Low, 1=Mid, 2=High)\n",
        "])\n",
        "optimizer = BO(bounds)\n",
        "\n",
        "print(\"--- Starting Random Warmup ---\")\n",
        "\n",
        "# 2. Random Warmup (Run 5 cheap experiments to start)\n",
        "for _ in range(5):\n",
        "    # Create random cheap input [Temp, pH, F1, F2, F3, 0]\n",
        "    x_random = np.array([np.random.uniform(30, 40), # Temp\n",
        "        np.random.uniform(6, 8),   # pH\n",
        "        np.random.uniform(0, 50),  # F1\n",
        "        np.random.uniform(0, 50),  # F2\n",
        "        np.random.uniform(0, 50),  # F3\n",
        "        0])                        # Fidelity=0 for cheap Warmup Loop\n",
        "    y_random = conduct_experiment([x_random])\n",
        "    optimizer.update_data(x_random, y_random)\n",
        "\n",
        "print(\"\\n--- Starting Main Optimization Loop ---\")\n",
        "\n",
        "# 3. The Main Loop\n",
        "while optimizer.budget > 10: # While we can afford at least a cheap run\n",
        "\n",
        "    # A. Ask the Brain for the next best recipe\n",
        "    x_next = optimizer.suggest_next_point()\n",
        "\n",
        "    # B. Check if we can afford it\n",
        "    fidelity = int(x_next[-1])\n",
        "    cost = optimizer.cost_map[fidelity]\n",
        "\n",
        "    if cost > optimizer.budget:\n",
        "        print(\"Cannot afford this fidelity anymore! Stopping or switching to cheap.\")\n",
        "        break\n",
        "\n",
        "    # C. Run the experiment\n",
        "    y_next = conduct_experiment([x_next]) # [cite: 18]\n",
        "\n",
        "    # D. Learn from it\n",
        "    optimizer.update_data(x_next, y_next)\n",
        "\n",
        "print(\"Optimization Done! Best Result:\", max(optimizer.Y))"
      ],
      "metadata": {
        "id": "Dy4nQiTXbZyh"
      },
      "id": "Dy4nQiTXbZyh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import gpytorch\n",
        "from botorch.models import SingleTaskGP\n",
        "from botorch.models.transforms import Normalize\n",
        "from botorch.fit import fit_gpytorch_mll\n",
        "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
        "\n",
        "# ==========================================\n",
        "# 1. THE GP CLASS (The \"Brain\")\n",
        "# ==========================================\n",
        "class GP:\n",
        "    def __init__(self, train_x, train_y):\n",
        "        \"\"\"\n",
        "        Wraps the BoTorch/GPyTorch model logic.\n",
        "        \"\"\"\n",
        "        # Convert numpy arrays to Torch Tensors (required for BoTorch)\n",
        "        self.train_x = torch.tensor(train_x).double()\n",
        "        self.train_y = torch.tensor(train_y).double()\n",
        "\n",
        "        # Check if Y is flat (dim=1). If so, make it 2D (N, 1).\n",
        "        if self.train_y.ndim == 1:\n",
        "            self.train_y = self.train_y.unsqueeze(-1)\n",
        "\n",
        "        # 2. FIX SCALING: Normalize inputs to [0, 1] range\n",
        "        # This silences the \"InputDataWarning\" and improves accuracy\n",
        "        # We assume 6 input dimensions (Temp, pH, F1, F2, F3, Fid)\n",
        "        # Initialize the Single Task Gaussian Process with Normalization\n",
        "        self.model = SingleTaskGP(\n",
        "            self.train_x,\n",
        "            self.train_y,\n",
        "            input_transform=Normalize(d=self.train_x.shape[-1]) # Normalization\n",
        "        )\n",
        "\n",
        "        self.mll = ExactMarginalLogLikelihood(self.model.likelihood, self.model)\n",
        "\n",
        "        # Initialize the Single Task Gaussian Process\n",
        "        self.model = SingleTaskGP(self.train_x, self.train_y)\n",
        "        self.mll = ExactMarginalLogLikelihood(self.model.likelihood, self.model)\n",
        "\n",
        "    def fit(self):\n",
        "        \"\"\"Trains the model on the current data.\"\"\"\n",
        "        fit_gpytorch_mll(self.mll)\n",
        "\n",
        "    def predict(self, candidates):\n",
        "        \"\"\"\n",
        "        Predicts Mean and StdDev for a list of candidate inputs.\n",
        "        Returns numpy arrays for easier handling.\n",
        "        \"\"\"\n",
        "        # Set to eval mode for prediction\n",
        "        self.model.eval()\n",
        "        candidate_tensor = torch.tensor(candidates).double()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            posterior = self.model.posterior(candidate_tensor)\n",
        "            mean = posterior.mean.squeeze().numpy()\n",
        "            std = posterior.variance.sqrt().squeeze().numpy()\n",
        "\n",
        "        return mean, std\n",
        "\n",
        "# ==========================================\n",
        "# 2. THE BO CLASS (The \"Optimizer\")\n",
        "# ==========================================\n",
        "class BO:\n",
        "    def __init__(self, bounds, budget=15000):\n",
        "        # REQUIRED ATTRIBUTES\n",
        "        self.X = [] # List to store inputs\n",
        "        self.Y = [] # List to store outputs\n",
        "\n",
        "        self.bounds = bounds\n",
        "        self.budget = budget\n",
        "        self.cost_map = {0: 10, 1: 575, 2: 2100}\n",
        "\n",
        "    def run_initial_warmup(self, n_points=5):\n",
        "        \"\"\"Runs cheap random experiments to get data for the GP.\"\"\"\n",
        "        print(\"--- Starting Warmup (Fidelity 0) ---\")\n",
        "        for _ in range(n_points):\n",
        "            # Generate random cheap point\n",
        "            x_random = [\n",
        "                np.random.uniform(self.bounds[0,0], self.bounds[0,1]), # Temp\n",
        "                np.random.uniform(self.bounds[1,0], self.bounds[1,1]), # pH\n",
        "                np.random.uniform(self.bounds[2,0], self.bounds[2,1]), # F1\n",
        "                np.random.uniform(self.bounds[3,0], self.bounds[3,1]), # F2\n",
        "                np.random.uniform(self.bounds[4,0], self.bounds[4,1]), # F3\n",
        "                0 # Force Low Fidelity\n",
        "            ]\n",
        "            # Run experiment\n",
        "            y_result = conduct_experiment([x_random])[0]\n",
        "\n",
        "            # Store it\n",
        "            self.update_data(x_random, y_result)\n",
        "\n",
        "    def update_data(self, x_new, y_new):\n",
        "        \"\"\"Updates internal memory and budget.\"\"\"\n",
        "        self.X.append(x_new)\n",
        "        self.Y.append(y_new)\n",
        "\n",
        "        fidelity = int(x_new[-1])\n",
        "        cost = self.cost_map[fidelity]\n",
        "        self.budget -= cost\n",
        "\n",
        "        print(f\"   -> Ran Fidelity {fidelity} (Cost: {cost}). Result: {y_new:.4f}. Budget Left: {self.budget}\")\n",
        "\n",
        "    def MF_lower_confidence_bound(self, gp_model, n_candidates=5000):\n",
        "        \"\"\"\n",
        "        ADVANCED GUIDANCE IMPLEMENTATION:\n",
        "        Calculates LCB and divides by Cost to prioritize cheap experiments.\n",
        "        \"\"\"\n",
        "        # 1. Create huge list of random candidates to test\n",
        "        candidates = np.random.uniform(\n",
        "            low=self.bounds[:, 0],\n",
        "            high=self.bounds[:, 1],\n",
        "            size=(n_candidates, 6)\n",
        "        )\n",
        "        # Fix Fidelity column to be integers 0, 1, or 2\n",
        "        candidates[:, -1] = np.random.randint(0, 3, size=n_candidates)\n",
        "\n",
        "        # 2. Ask GP for predictions\n",
        "        mean, std = gp_model.predict(candidates)\n",
        "\n",
        "        # 3. Calculate LCB (Lower Confidence Bound)\n",
        "        # We minimize negative titre.\n",
        "        beta = 1.96\n",
        "        lower_std = mean - (beta * std)\n",
        "\n",
        "        # 4. COST AWARE SCALING (The Magic Step)\n",
        "        # mf_lower_std = lower_std / associated cost\n",
        "        costs = np.array([self.cost_map[int(f)] for f in candidates[:, -1]])\n",
        "        mf_lower_std = lower_std / costs\n",
        "\n",
        "        # 5. Pick the \"most negative\" value\n",
        "        best_index = np.argmin(mf_lower_std)\n",
        "        return candidates[best_index]\n",
        "\n",
        "    def optimize(self):\n",
        "        \"\"\"The Main Loop\"\"\"\n",
        "        self.run_initial_warmup()\n",
        "\n",
        "        while self.budget > 10:\n",
        "            # SAFETY CHECK: If budget is getting low (near 2500),\n",
        "            # force a High Fidelity run and stop.\n",
        "            if self.budget < 2500:\n",
        "                print(\"!!! BUDGET LOW - FORCING FINAL HIGH FIDELITY RUN !!!\")\n",
        "                # Pick best point found so far, but upgrade it to Fidelity 2\n",
        "                best_idx = np.argmin(self.Y) # Minimizing negative = best yield\n",
        "                final_x = list(self.X[best_idx])\n",
        "                final_x[-1] = 2 # Force High Fidelity\n",
        "\n",
        "                # Check if we can afford it (just in case)\n",
        "                if self.budget >= 2100:\n",
        "                    y_res = conduct_experiment([final_x])[0]\n",
        "                    self.update_data(final_x, y_res)\n",
        "                break\n",
        "\n",
        "            # 1. Instantiate and Fit GP\n",
        "            gp = GP(self.X, self.Y)\n",
        "            gp.fit()\n",
        "\n",
        "            # 2. Get next suggestion using Cost-Aware LCB\n",
        "            x_next = self.MF_lower_confidence_bound(gp)\n",
        "\n",
        "            # 3. Check Budget\n",
        "            fidelity = int(x_next[-1])\n",
        "            cost = self.cost_map[fidelity]\n",
        "\n",
        "            if cost > self.budget:\n",
        "                print(\"Too expensive! Skipping this suggestion.\")\n",
        "                break\n",
        "\n",
        "            # 4. Run Experiment\n",
        "            y_next = conduct_experiment([x_next])[0]\n",
        "            self.update_data(x_next, y_next)\n",
        "\n",
        "# ==========================================\n",
        "# 3. BO EXECUTION BLOCK\n",
        "# ==========================================\n",
        "\n",
        "# Define Search Space (Bounds)\n",
        "# Temp(30-40), pH(6-8), F1(0-50), F2(0-50), F3(0-50), Fid(0-2)\n",
        "X_searchspace = np.array([\n",
        "    [30, 40], [6, 8], [0, 50], [0, 50], [0, 50], [0, 2]\n",
        "])\n",
        "\n",
        "# Initialize the Optimizer\n",
        "BO_m = BO(X_searchspace, budget=15000)\n",
        "\n",
        "# Run the Optimization\n",
        "BO_m.optimize()\n",
        "\n",
        "# Final Report\n",
        "print(\"\\n--- OPTIMIZATION COMPLETE ---\")\n",
        "# Remember to convert back to positive for reading (since we minimized negative)\n",
        "best_titre = -1 * min(BO_m.Y)\n",
        "print(f\"Highest Titre Found: {best_titre:.4f} g/L\")"
      ],
      "metadata": {
        "id": "NRi0HNJXoWI4"
      },
      "id": "NRi0HNJXoWI4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import gpytorch\n",
        "from botorch.models import SingleTaskGP\n",
        "from botorch.models.transforms import Normalize\n",
        "from botorch import fit_gpytorch_mll\n",
        "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
        "from scipy.stats import norm # <--- NEW IMPORT FOR MATH\n",
        "\n",
        "# ==========================================\n",
        "# 1. THE GP CLASS\n",
        "# ==========================================\n",
        "class GP:\n",
        "    def __init__(self, train_x, train_y):\n",
        "        self.train_x = torch.tensor(train_x).double()\n",
        "        self.train_y = torch.tensor(train_y).double()\n",
        "        if self.train_y.ndim == 1:\n",
        "            self.train_y = self.train_y.unsqueeze(-1)\n",
        "\n",
        "        self.model = SingleTaskGP(\n",
        "            self.train_x,\n",
        "            self.train_y,\n",
        "            input_transform=Normalize(d=self.train_x.shape[-1])\n",
        "        )\n",
        "        self.mll = ExactMarginalLogLikelihood(self.model.likelihood, self.model)\n",
        "\n",
        "    def fit(self):\n",
        "        fit_gpytorch_mll(self.mll)\n",
        "\n",
        "    def predict(self, candidates):\n",
        "        self.model.eval()\n",
        "        candidate_tensor = torch.tensor(candidates).double()\n",
        "        with torch.no_grad():\n",
        "            posterior = self.model.posterior(candidate_tensor)\n",
        "            mean = posterior.mean.squeeze().numpy()\n",
        "            std = posterior.variance.sqrt().squeeze().numpy()\n",
        "        return mean, std\n",
        "\n",
        "# ==========================================\n",
        "# 2. THE BO CLASS (Fixed with Expected Improvement)\n",
        "# ==========================================\n",
        "class BO:\n",
        "    def __init__(self, bounds, budget=15000):\n",
        "        self.X = []\n",
        "        self.Y = []\n",
        "        self.bounds = bounds\n",
        "        self.budget = budget\n",
        "        self.cost_map = {0: 10, 1: 575, 2: 2100}\n",
        "\n",
        "    def run_initial_warmup(self, n_points=5):\n",
        "        print(\"--- Starting Warmup (Fidelity 0) ---\")\n",
        "        for i in range(n_points):\n",
        "            # 1. Force a new random seed just in case\n",
        "            np.random.seed(None)\n",
        "\n",
        "            # 2. Generate the random recipe FRESH inside the loop\n",
        "            x_random = [\n",
        "                float(np.random.uniform(self.bounds[0,0], self.bounds[0,1])), # Temp\n",
        "                float(np.random.uniform(self.bounds[1,0], self.bounds[1,1])), # pH\n",
        "                float(np.random.uniform(self.bounds[2,0], self.bounds[2,1])), # F1\n",
        "                float(np.random.uniform(self.bounds[3,0], self.bounds[3,1])), # F2\n",
        "                float(np.random.uniform(self.bounds[4,0], self.bounds[4,1])), # F3\n",
        "                0.0 # Force Low Fidelity (Float)\n",
        "            ]\n",
        "\n",
        "            # 3. PRINT THE INPUT (The \"Sanity Check\")\n",
        "            # If these numbers look the same in your output, Python is broken!\n",
        "            print(f\"   [Warmup {i+1}] Testing Recipe: {x_random[:2]}...\")\n",
        "\n",
        "            # 4. Run experiment\n",
        "            # IMPORTANT: We wrap x_random in a LIST -> [x_random]\n",
        "            # This matches how you ran the manual test successfully!\n",
        "            y_result = conduct_experiment([x_random])[0]\n",
        "\n",
        "            # 5. Store it\n",
        "            self.update_data(x_random, y_result)\n",
        "\n",
        "    def update_data(self, x_new, y_new):\n",
        "        if isinstance(x_new, np.ndarray): x_new = x_new.tolist()\n",
        "        self.X.append(x_new)\n",
        "        # Store negative yield for minimization\n",
        "        self.Y.append(-1 * y_new)\n",
        "\n",
        "        fidelity = int(x_new[-1])\n",
        "        cost = self.cost_map[fidelity]\n",
        "        self.budget -= cost\n",
        "        print(f\"   -> Ran Fidelity {fidelity} (Cost: {cost}). Result: {y_new:.4f} g/L. Budget Left: {self.budget}\")\n",
        "\n",
        "    def MF_expected_improvement(self, gp_model, n_candidates=5000):\n",
        "        \"\"\"\n",
        "        Calculates Expected Improvement (EI) scaled by Cost.\n",
        "        This prevents the AI from getting stuck on the same point.\n",
        "        \"\"\"\n",
        "        # 1. Generate Candidates\n",
        "        candidates = np.random.uniform(low=self.bounds[:, 0], high=self.bounds[:, 1], size=(n_candidates, 6))\n",
        "        candidates[:, -1] = np.random.randint(0, 3, size=n_candidates)\n",
        "\n",
        "        # 2. Predict Mean and Std\n",
        "        mean, std = gp_model.predict(candidates)\n",
        "\n",
        "        # 3. Calculate EI (The Manual Math Way)\n",
        "        # We are minimizing negative titre. Best so far is the lowest value in self.Y\n",
        "        best_f = np.min(self.Y)\n",
        "        xi = 0.01 # Jitter (Exploration parameter)\n",
        "\n",
        "        # Z-score calculation\n",
        "        with np.errstate(divide='warn'):\n",
        "            imp = best_f - mean - xi\n",
        "            Z = imp / std\n",
        "            # If std is 0 (we already visited this point), EI should be 0\n",
        "            ei = imp * norm.cdf(Z) + std * norm.pdf(Z)\n",
        "            ei[std == 0.0] = 0.0\n",
        "\n",
        "        # 4. Cost Scaling (Maximize Bang for Buck)\n",
        "        # EI is positive \"Gain\". We want to Maximize (Gain / Cost).\n",
        "        costs = np.array([self.cost_map[int(f)] for f in candidates[:, -1]])\n",
        "\n",
        "        # We add a tiny epsilon to cost to avoid divide by zero (just in case)\n",
        "        score = ei / costs\n",
        "\n",
        "        # 5. Pick the Winner (Argmax because we want HIGHEST efficiency)\n",
        "        best_index = np.argmax(score)\n",
        "        return candidates[best_index]\n",
        "\n",
        "    def optimize(self):\n",
        "        self.run_initial_warmup()\n",
        "\n",
        "        while self.budget > 10:\n",
        "            # SAFETY CHECK: If budget is low, force High Fid verification\n",
        "            if self.budget < 2700:\n",
        "                print(\"!!! BUDGET LOW - FORCING FINAL HIGH FIDELITY RUN !!!\")\n",
        "                best_idx = np.argmin(self.Y)\n",
        "                final_x = list(self.X[best_idx])\n",
        "\n",
        "                # Only run if we haven't already verified this exact spot at High Fid\n",
        "                if final_x[-1] != 2:\n",
        "                    final_x[-1] = 2\n",
        "                    if self.budget >= 2100:\n",
        "                        y_res = conduct_experiment([final_x])[0]\n",
        "                        self.update_data(final_x, y_res)\n",
        "                break\n",
        "\n",
        "            # 1. Fit GP\n",
        "            gp = GP(self.X, self.Y)\n",
        "            gp.fit()\n",
        "\n",
        "            # 2. Get suggestion using EI / Cost\n",
        "            x_next = self.MF_expected_improvement(gp)\n",
        "\n",
        "            # 3. Check Budget\n",
        "            cost = self.cost_map[int(x_next[-1])]\n",
        "            if cost > self.budget:\n",
        "                print(\"Too expensive! Skipping.\")\n",
        "                break\n",
        "\n",
        "            y_next = conduct_experiment([x_next])[0]\n",
        "            self.update_data(x_next, y_next)\n",
        "\n",
        "# ==========================================\n",
        "# 3. EXECUTION\n",
        "# ==========================================\n",
        "X_searchspace = np.array([[30, 40], [6, 8], [0, 50], [0, 50], [0, 50], [0, 2]])\n",
        "\n",
        "BO_m = BO(X_searchspace, budget=15000)\n",
        "BO_m.optimize()\n",
        "\n",
        "# --- FINAL REPORT ---\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\"       OPTIMIZATION REPORT       \")\n",
        "print(\"=\"*40)\n",
        "\n",
        "# 1. Separate results by Fidelity\n",
        "results = []\n",
        "for x, y in zip(BO_m.X, BO_m.Y):\n",
        "    # Remember: y is stored as negative, so flip it back\n",
        "    results.append({'fidelity': int(x[-1]), 'titre': -1 * y})\n",
        "\n",
        "# 2. Find Best Cheap vs Best Expensive\n",
        "fid0_results = [r['titre'] for r in results if r['fidelity'] == 0]\n",
        "fid2_results = [r['titre'] for r in results if r['fidelity'] == 2]\n",
        "\n",
        "best_fid0 = max(fid0_results) if fid0_results else 0.0\n",
        "best_fid2 = max(fid2_results) if fid2_results else 0.0\n",
        "\n",
        "print(f\"Total Experiments:   {len(results)}\")\n",
        "print(f\"High Fidelity Runs:  {len(fid2_results)}\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"Best Scout (Fid 0):  {best_fid0:.4f} g/L\")\n",
        "print(f\"Best Real (Fid 2):   {best_fid2:.4f} g/L\")  # <--- THIS IS YOUR SCORE\n",
        "print(\"=\"*40)"
      ],
      "metadata": {
        "id": "HD52Q3AeKFrI"
      },
      "id": "HD52Q3AeKFrI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import gpytorch\n",
        "from botorch.models import SingleTaskGP\n",
        "from botorch.models.transforms import Normalize\n",
        "from botorch import fit_gpytorch_mll\n",
        "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
        "from scipy.stats import norm # <--- NEW IMPORT FOR MATH\n",
        "\n",
        "# ==========================================\n",
        "# 1. THE GP CLASS\n",
        "# ==========================================\n",
        "class GP:\n",
        "    def __init__(self, train_x, train_y):\n",
        "        self.train_x = torch.tensor(train_x).double()\n",
        "        self.train_y = torch.tensor(train_y).double()\n",
        "        if self.train_y.ndim == 1:\n",
        "            self.train_y = self.train_y.unsqueeze(-1)\n",
        "\n",
        "        self.model = SingleTaskGP(\n",
        "            self.train_x,\n",
        "            self.train_y,\n",
        "            input_transform=Normalize(d=self.train_x.shape[-1])\n",
        "        )\n",
        "        self.mll = ExactMarginalLogLikelihood(self.model.likelihood, self.model)\n",
        "\n",
        "    def fit(self):\n",
        "        fit_gpytorch_mll(self.mll)\n",
        "\n",
        "    def predict(self, candidates):\n",
        "        self.model.eval()\n",
        "        candidate_tensor = torch.tensor(candidates).double()\n",
        "        with torch.no_grad():\n",
        "            posterior = self.model.posterior(candidate_tensor)\n",
        "            mean = posterior.mean.squeeze().numpy()\n",
        "            std = posterior.variance.sqrt().squeeze().numpy()\n",
        "        return mean, std\n",
        "\n",
        "# ==========================================\n",
        "# 2. THE BO CLASS (Upgraded: Balanced Spending)\n",
        "# ==========================================\n",
        "class BO:\n",
        "    def __init__(self, bounds, budget=15000):\n",
        "        self.X = []\n",
        "        self.Y = []\n",
        "        self.bounds = bounds\n",
        "        self.budget = budget\n",
        "        self.cost_map = {0: 10, 1: 575, 2: 2100}\n",
        "\n",
        "    def run_initial_warmup(self, n_points=20):\n",
        "        print(f\"--- Starting Robust Warmup ({n_points} points) ---\")\n",
        "\n",
        "        # We use Python's native 'random' library to bypass NumPy/BoTorch seed locks\n",
        "\n",
        "        for i in range(n_points):\n",
        "            x_random = [\n",
        "                random.uniform(self.bounds[0,0], self.bounds[0,1]), # Temp\n",
        "                random.uniform(self.bounds[1,0], self.bounds[1,1]), # pH\n",
        "                random.uniform(self.bounds[2,0], self.bounds[2,1]), # F1\n",
        "                random.uniform(self.bounds[3,0], self.bounds[3,1]), # F2\n",
        "                random.uniform(self.bounds[4,0], self.bounds[4,1]), # F3\n",
        "                0.0 # Force Low Fidelity\n",
        "            ]\n",
        "\n",
        "            # Print to confirm they are changing\n",
        "            print(f\"   [Warmup {i+1}] Try: {x_random[0]:.2f} C, {x_random[1]:.2f} pH\")\n",
        "\n",
        "            # Run experiment\n",
        "            y_result = conduct_experiment([x_random])[0]\n",
        "            self.update_data(x_random, y_result)\n",
        "\n",
        "        print(f\"--- Warmup Complete. Best Scout Found: {-1 * min(self.Y):.4f} g/L ---\")\n",
        "\n",
        "    def update_data(self, x_new, y_new):\n",
        "        if isinstance(x_new, np.ndarray): x_new = x_new.tolist()\n",
        "        self.X.append(x_new)\n",
        "        self.Y.append(-1 * y_new) # Minimize Negative\n",
        "\n",
        "        fidelity = int(x_new[-1])\n",
        "        cost = self.cost_map[fidelity]\n",
        "        self.budget -= cost\n",
        "\n",
        "        # --- NEW LABELING LOGIC ---\n",
        "        # The length of X is the current experiment number (1-based counting)\n",
        "        exp_id = len(self.X)\n",
        "\n",
        "        print(f\"   [Exp #{exp_id}] Ran Fidelity {fidelity} (Cost: {cost}). Result: {y_new:.4f} g/L. Budget Left: {self.budget}\")\n",
        "\n",
        "    def MF_expected_improvement(self, gp_model, n_candidates=5000):\n",
        "        \"\"\"\n",
        "        Modified to encourage Mid/High Fidelity runs using 'Cost Smoothing'.\n",
        "        \"\"\"\n",
        "        # 1. Generate Candidates\n",
        "        candidates = np.random.uniform(low=self.bounds[:, 0], high=self.bounds[:, 1], size=(n_candidates, 6))\n",
        "        candidates[:, -1] = np.random.randint(0, 3, size=n_candidates)\n",
        "\n",
        "        # 2. Predict\n",
        "        mean, std = gp_model.predict(candidates)\n",
        "\n",
        "        # 3. Calculate EI\n",
        "        best_f = np.min(self.Y)\n",
        "        xi = 0.01\n",
        "\n",
        "        with np.errstate(divide='warn'):\n",
        "            imp = best_f - mean - xi\n",
        "            Z = imp / std\n",
        "            ei = imp * norm.cdf(Z) + std * norm.pdf(Z)\n",
        "            ei[std == 0.0] = 0.0\n",
        "\n",
        "        # 4. COST SMOOTHING (The Fix)\n",
        "        costs = np.array([self.cost_map[int(f)] for f in candidates[:, -1]])\n",
        "\n",
        "        # OLD WAY: score = ei / costs\n",
        "        score = ei / costs\n",
        "        # NEW WAY: score = ei / sqrt(costs)\n",
        "        # This reduces the penalty for expensive runs.\n",
        "        # Fid 0 Penalty: 3.1 (was 10)\n",
        "        # Fid 1 Penalty: 24.0 (was 575) -> Huge improvement!\n",
        "        # Fid 2 Penalty: 45.8 (was 2100) -> Massive improvement!\n",
        "        # score = ei / np.power(costs, 0.5)\n",
        "\n",
        "        best_index = np.argmax(score)\n",
        "        return candidates[best_index]\n",
        "\n",
        "    def optimize(self):\n",
        "        self.run_initial_warmup()\n",
        "\n",
        "        while self.budget > 10:\n",
        "            # --- THE GRAND FINALE ---\n",
        "            if self.budget < 6310:\n",
        "                print(\"\\n!!! ENTERING FINAL VERIFICATION PHASE (Top 3 Candidates) !!!\")\n",
        "\n",
        "                # Sort indices by best result (minimizing negative = best)\n",
        "                sorted_indices = np.argsort(self.Y)\n",
        "                runs_done = 0\n",
        "\n",
        "                for rank, idx in enumerate(sorted_indices):\n",
        "                    if runs_done >= 3: break\n",
        "                    if self.budget < 2100: break\n",
        "\n",
        "                    candidate = list(self.X[idx])\n",
        "\n",
        "                    # Only verify if we haven't run High Fidelity yet\n",
        "                    if candidate[-1] != 2:\n",
        "                        candidate[-1] = 2\n",
        "\n",
        "                        # Duplicate Check\n",
        "                        is_already_run = False\n",
        "                        for past_x in self.X:\n",
        "                            if np.linalg.norm(np.array(candidate) - np.array(past_x)) < 1e-3:\n",
        "                                is_already_run = True\n",
        "                                break\n",
        "\n",
        "                        if not is_already_run:\n",
        "                            # --- MATCHING ID LOGIC ---\n",
        "                            # We use (idx + 1) so it matches the [Exp #] log from earlier\n",
        "                            print(f\"   > Verifying Rank #{rank+1} Candidate (Source: Exp #{idx+1})...\")\n",
        "\n",
        "                            y_res = conduct_experiment([candidate])[0]\n",
        "                            self.update_data(candidate, y_res)\n",
        "                            runs_done += 1\n",
        "                break\n",
        "\n",
        "            # 1. Fit GP\n",
        "            gp = GP(self.X, self.Y)\n",
        "            gp.fit()\n",
        "\n",
        "            # 2. Get Suggestion\n",
        "            x_next = self.MF_expected_improvement(gp)\n",
        "\n",
        "            # --- PHASE 1: THE \"EARLY BAN\" ---\n",
        "            # If we haven't done 50 experiments yet, BAN Fidelity 2.\n",
        "            # We force it down to Fidelity 1 (High Quality Scout) or 0 (Cheap).\n",
        "            # This prevents \"Premature Verification\".\n",
        "            if len(self.X) < 50:\n",
        "                if x_next[-1] == 2.0:\n",
        "                    # Downgrade to Fidelity 1 (Middle Ground)\n",
        "                    # This gives us accurate data without the 2100 price tag.\n",
        "                    print(\"   -> Early Stage (Exp < 50): Downgrading Fid 2 to Fid 1.\")\n",
        "                    x_next[-1] = 1.0\n",
        "\n",
        "            # 3. Duplicate Detector (Kick Logic)\n",
        "            is_duplicate = False\n",
        "            x_next_params = np.array(x_next[:-1])\n",
        "            for past_x in self.X:\n",
        "                past_params = np.array(past_x[:-1])\n",
        "                if np.linalg.norm(x_next_params - past_params) < 0.5:\n",
        "                    is_duplicate = True\n",
        "                    break\n",
        "\n",
        "            if is_duplicate:\n",
        "                print(\"   -> AI stuck on duplicate. KICKING to random spot!\")\n",
        "\n",
        "                # 1. Use Python's native 'random' to avoid NumPy seed locks\n",
        "                # Randomly pick Fidelity 0 or 1\n",
        "                random_fid = float(random.choice([0.0, 1.0]))\n",
        "\n",
        "                # 2. Generate a fresh random point\n",
        "                x_next = [\n",
        "                    random.uniform(self.bounds[0,0], self.bounds[0,1]), # Temp\n",
        "                    random.uniform(self.bounds[1,0], self.bounds[1,1]), # pH\n",
        "                    random.uniform(self.bounds[2,0], self.bounds[2,1]), # F1\n",
        "                    random.uniform(self.bounds[3,0], self.bounds[3,1]), # F2\n",
        "                    random.uniform(self.bounds[4,0], self.bounds[4,1]), # F3\n",
        "                    random_fid\n",
        "                ]\n",
        "\n",
        "            # 4. Check Budget & Run\n",
        "            cost = self.cost_map[int(x_next[-1])]\n",
        "\n",
        "            # Downgrade logic if saving for Finale\n",
        "            if self.budget - cost < 6310:\n",
        "                x_next[-1] = 0.0\n",
        "                cost = 10\n",
        "\n",
        "            y_next = conduct_experiment([x_next])[0]\n",
        "            self.update_data(x_next, y_next)\n",
        "\n",
        "# ==========================================\n",
        "# 3. EXECUTION\n",
        "# ==========================================\n",
        "X_searchspace = np.array([[30, 40], [6, 8], [0, 50], [0, 50], [0, 50], [0, 2]])\n",
        "\n",
        "BO_m = BO(X_searchspace, budget=15000)\n",
        "BO_m.optimize()\n",
        "\n",
        "# --- FINAL REPORT ---\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\"       OPTIMIZATION REPORT       \")\n",
        "print(\"=\"*40)\n",
        "\n",
        "# 1. Separate results by Fidelity\n",
        "results = []\n",
        "for x, y in zip(BO_m.X, BO_m.Y):\n",
        "    # Remember: y is stored as negative, so flip it back\n",
        "    results.append({'fidelity': int(x[-1]), 'titre': -1 * y})\n",
        "\n",
        "# 2. Extract Results\n",
        "fid0_results = [r['titre'] for r in results if r['fidelity'] == 0]\n",
        "fid1_results = [r['titre'] for r in results if r['fidelity'] == 1]\n",
        "fid2_results = [r['titre'] for r in results if r['fidelity'] == 2]\n",
        "\n",
        "# 3. Find Bests\n",
        "best_fid0 = max(fid0_results) if fid0_results else 0.0\n",
        "best_fid1 = max(fid1_results) if fid1_results else 0.0\n",
        "best_fid2 = max(fid2_results) if fid2_results else 0.0\n",
        "\n",
        "print(f\"Total Experiments:   {len(results)}\")\n",
        "print(f\" - Low Fid (0) Runs: {len(fid0_results)}\")\n",
        "print(f\" - Mid Fid (1) Runs: {len(fid1_results)}\")\n",
        "print(f\" - High Fid (2) Runs:{len(fid2_results)}\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"Best Basic Scout (Fid 0):   {best_fid0:.4f} g/L\")\n",
        "print(f\"Best Adv. Scout (Fid 1):    {best_fid1:.4f} g/L\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"🏆 BEST REAL RESULT (Fid 2): {best_fid2:.4f} g/L\")\n",
        "print(\"=\"*40)"
      ],
      "metadata": {
        "id": "aQ4z1KLsmQas",
        "outputId": "fcdfd186-4915-46c2-9d0b-a969c56fc455",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "aQ4z1KLsmQas",
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Robust Warmup (20 points) ---\n",
            "   [Warmup 1] Try: 39.59 C, 6.63 pH\n",
            "   [Exp #1] Ran Fidelity 0 (Cost: 10). Result: 1.3635 g/L. Budget Left: 14990\n",
            "   [Warmup 2] Try: 39.20 C, 7.66 pH\n",
            "   [Exp #2] Ran Fidelity 0 (Cost: 10). Result: 1.3634 g/L. Budget Left: 14980\n",
            "   [Warmup 3] Try: 38.49 C, 6.84 pH\n",
            "   [Exp #3] Ran Fidelity 0 (Cost: 10). Result: 1.3687 g/L. Budget Left: 14970\n",
            "   [Warmup 4] Try: 34.77 C, 6.12 pH\n",
            "   [Exp #4] Ran Fidelity 0 (Cost: 10). Result: 8.5541 g/L. Budget Left: 14960\n",
            "   [Warmup 5] Try: 33.55 C, 7.36 pH\n",
            "   [Exp #5] Ran Fidelity 0 (Cost: 10). Result: 20.1598 g/L. Budget Left: 14950\n",
            "   [Warmup 6] Try: 38.12 C, 7.63 pH\n",
            "   [Exp #6] Ran Fidelity 0 (Cost: 10). Result: 1.3660 g/L. Budget Left: 14940\n",
            "   [Warmup 7] Try: 31.50 C, 7.71 pH\n",
            "   [Exp #7] Ran Fidelity 0 (Cost: 10). Result: 10.4256 g/L. Budget Left: 14930\n",
            "   [Warmup 8] Try: 36.43 C, 7.45 pH\n",
            "   [Exp #8] Ran Fidelity 0 (Cost: 10). Result: 5.7872 g/L. Budget Left: 14920\n",
            "   [Warmup 9] Try: 36.92 C, 7.47 pH\n",
            "   [Exp #9] Ran Fidelity 0 (Cost: 10). Result: 1.7925 g/L. Budget Left: 14910\n",
            "   [Warmup 10] Try: 37.08 C, 6.45 pH\n",
            "   [Exp #10] Ran Fidelity 0 (Cost: 10). Result: 1.4528 g/L. Budget Left: 14900\n",
            "   [Warmup 11] Try: 37.14 C, 7.21 pH\n",
            "   [Exp #11] Ran Fidelity 0 (Cost: 10). Result: 2.3071 g/L. Budget Left: 14890\n",
            "   [Warmup 12] Try: 38.80 C, 6.49 pH\n",
            "   [Exp #12] Ran Fidelity 0 (Cost: 10). Result: 1.3648 g/L. Budget Left: 14880\n",
            "   [Warmup 13] Try: 32.87 C, 7.51 pH\n",
            "   [Exp #13] Ran Fidelity 0 (Cost: 10). Result: 17.3365 g/L. Budget Left: 14870\n",
            "   [Warmup 14] Try: 37.64 C, 7.63 pH\n",
            "   [Exp #14] Ran Fidelity 0 (Cost: 10). Result: 1.3705 g/L. Budget Left: 14860\n",
            "   [Warmup 15] Try: 30.17 C, 7.82 pH\n",
            "   [Exp #15] Ran Fidelity 0 (Cost: 10). Result: 4.8503 g/L. Budget Left: 14850\n",
            "   [Warmup 16] Try: 31.22 C, 6.23 pH\n",
            "   [Exp #16] Ran Fidelity 0 (Cost: 10). Result: 16.1970 g/L. Budget Left: 14840\n",
            "   [Warmup 17] Try: 38.84 C, 7.21 pH\n",
            "   [Exp #17] Ran Fidelity 0 (Cost: 10). Result: 1.3664 g/L. Budget Left: 14830\n",
            "   [Warmup 18] Try: 30.27 C, 6.75 pH\n",
            "   [Exp #18] Ran Fidelity 0 (Cost: 10). Result: 11.5731 g/L. Budget Left: 14820\n",
            "   [Warmup 19] Try: 39.58 C, 7.92 pH\n",
            "   [Exp #19] Ran Fidelity 0 (Cost: 10). Result: 1.3629 g/L. Budget Left: 14810\n",
            "   [Warmup 20] Try: 34.57 C, 6.05 pH\n",
            "   [Exp #20] Ran Fidelity 0 (Cost: 10). Result: 8.3733 g/L. Budget Left: 14800\n",
            "--- Warmup Complete. Best Scout Found: 20.1598 g/L ---\n",
            "   [Exp #21] Ran Fidelity 0 (Cost: 10). Result: 8.1664 g/L. Budget Left: 14790\n",
            "   [Exp #22] Ran Fidelity 0 (Cost: 10). Result: 21.5163 g/L. Budget Left: 14780\n",
            "   [Exp #23] Ran Fidelity 0 (Cost: 10). Result: 21.5620 g/L. Budget Left: 14770\n",
            "   -> AI stuck on duplicate. KICKING to random spot!\n",
            "   [Exp #24] Ran Fidelity 0 (Cost: 10). Result: 1.3648 g/L. Budget Left: 14760\n",
            "   -> AI stuck on duplicate. KICKING to random spot!\n",
            "   [Exp #25] Ran Fidelity 1 (Cost: 575). Result: 1.3296 g/L. Budget Left: 14185\n",
            "   -> AI stuck on duplicate. KICKING to random spot!\n",
            "   [Exp #26] Ran Fidelity 1 (Cost: 575). Result: 1.2950 g/L. Budget Left: 13610\n",
            "   -> AI stuck on duplicate. KICKING to random spot!\n",
            "   [Exp #27] Ran Fidelity 1 (Cost: 575). Result: 1.8123 g/L. Budget Left: 13035\n",
            "   -> AI stuck on duplicate. KICKING to random spot!\n",
            "   [Exp #28] Ran Fidelity 0 (Cost: 10). Result: 5.9249 g/L. Budget Left: 13025\n",
            "   [Exp #29] Ran Fidelity 0 (Cost: 10). Result: 20.8700 g/L. Budget Left: 13015\n",
            "   -> AI stuck on duplicate. KICKING to random spot!\n",
            "   [Exp #30] Ran Fidelity 1 (Cost: 575). Result: 3.5367 g/L. Budget Left: 12440\n",
            "   -> AI stuck on duplicate. KICKING to random spot!\n",
            "   [Exp #31] Ran Fidelity 0 (Cost: 10). Result: 7.0259 g/L. Budget Left: 12430\n",
            "   -> AI stuck on duplicate. KICKING to random spot!\n",
            "   [Exp #32] Ran Fidelity 1 (Cost: 575). Result: 1.2934 g/L. Budget Left: 11855\n",
            "   -> AI stuck on duplicate. KICKING to random spot!\n",
            "   [Exp #33] Ran Fidelity 0 (Cost: 10). Result: 9.1556 g/L. Budget Left: 11845\n",
            "   -> AI stuck on duplicate. KICKING to random spot!\n",
            "   [Exp #34] Ran Fidelity 1 (Cost: 575). Result: 20.5521 g/L. Budget Left: 11270\n",
            "   [Exp #35] Ran Fidelity 0 (Cost: 10). Result: 21.6861 g/L. Budget Left: 11260\n",
            "   [Exp #36] Ran Fidelity 0 (Cost: 10). Result: 19.6087 g/L. Budget Left: 11250\n",
            "   [Exp #37] Ran Fidelity 0 (Cost: 10). Result: 21.1543 g/L. Budget Left: 11240\n",
            "   [Exp #38] Ran Fidelity 0 (Cost: 10). Result: 21.2190 g/L. Budget Left: 11230\n",
            "   -> AI stuck on duplicate. KICKING to random spot!\n",
            "   [Exp #39] Ran Fidelity 1 (Cost: 575). Result: 29.1809 g/L. Budget Left: 10655\n",
            "   [Exp #40] Ran Fidelity 1 (Cost: 575). Result: 23.7165 g/L. Budget Left: 10080\n",
            "   [Exp #41] Ran Fidelity 1 (Cost: 575). Result: 77.3868 g/L. Budget Left: 9505\n",
            "   [Exp #42] Ran Fidelity 1 (Cost: 575). Result: 20.9868 g/L. Budget Left: 8930\n",
            "   [Exp #43] Ran Fidelity 1 (Cost: 575). Result: 135.9694 g/L. Budget Left: 8355\n",
            "   [Exp #44] Ran Fidelity 1 (Cost: 575). Result: 32.7164 g/L. Budget Left: 7780\n",
            "   [Exp #45] Ran Fidelity 1 (Cost: 575). Result: 83.0023 g/L. Budget Left: 7205\n",
            "   [Exp #46] Ran Fidelity 1 (Cost: 575). Result: 62.9072 g/L. Budget Left: 6630\n",
            "   [Exp #47] Ran Fidelity 0 (Cost: 10). Result: 19.8555 g/L. Budget Left: 6620\n",
            "   -> AI stuck on duplicate. KICKING to random spot!\n",
            "   [Exp #48] Ran Fidelity 0 (Cost: 10). Result: 1.3703 g/L. Budget Left: 6610\n",
            "   -> AI stuck on duplicate. KICKING to random spot!\n",
            "   [Exp #49] Ran Fidelity 0 (Cost: 10). Result: 6.8251 g/L. Budget Left: 6600\n",
            "   -> AI stuck on duplicate. KICKING to random spot!\n",
            "   [Exp #50] Ran Fidelity 0 (Cost: 10). Result: 12.9202 g/L. Budget Left: 6590\n",
            "   -> AI stuck on duplicate. KICKING to random spot!\n",
            "   [Exp #51] Ran Fidelity 0 (Cost: 10). Result: 12.8002 g/L. Budget Left: 6580\n",
            "   -> AI stuck on duplicate. KICKING to random spot!\n",
            "   [Exp #52] Ran Fidelity 0 (Cost: 10). Result: 3.1088 g/L. Budget Left: 6570\n",
            "   -> AI stuck on duplicate. KICKING to random spot!\n",
            "   [Exp #53] Ran Fidelity 0 (Cost: 10). Result: 1.3636 g/L. Budget Left: 6560\n",
            "   -> AI stuck on duplicate. KICKING to random spot!\n",
            "   [Exp #54] Ran Fidelity 0 (Cost: 10). Result: 1.3644 g/L. Budget Left: 6550\n",
            "   -> AI stuck on duplicate. KICKING to random spot!\n",
            "   [Exp #55] Ran Fidelity 0 (Cost: 10). Result: 18.2177 g/L. Budget Left: 6540\n",
            "   -> AI stuck on duplicate. KICKING to random spot!\n",
            "   [Exp #56] Ran Fidelity 0 (Cost: 10). Result: 9.8740 g/L. Budget Left: 6530\n",
            "   -> AI stuck on duplicate. KICKING to random spot!\n",
            "   [Exp #57] Ran Fidelity 0 (Cost: 10). Result: 14.4297 g/L. Budget Left: 6520\n",
            "   -> AI stuck on duplicate. KICKING to random spot!\n",
            "   [Exp #58] Ran Fidelity 0 (Cost: 10). Result: 13.8222 g/L. Budget Left: 6510\n",
            "   -> AI stuck on duplicate. KICKING to random spot!\n",
            "   [Exp #59] Ran Fidelity 0 (Cost: 10). Result: 15.0517 g/L. Budget Left: 6500\n",
            "   -> AI stuck on duplicate. KICKING to random spot!\n",
            "   [Exp #60] Ran Fidelity 0 (Cost: 10). Result: 1.3636 g/L. Budget Left: 6490\n",
            "   -> AI stuck on duplicate. KICKING to random spot!\n",
            "   [Exp #61] Ran Fidelity 0 (Cost: 10). Result: 1.3760 g/L. Budget Left: 6480\n",
            "   -> AI stuck on duplicate. KICKING to random spot!\n",
            "   [Exp #62] Ran Fidelity 0 (Cost: 10). Result: 7.6900 g/L. Budget Left: 6470\n",
            "   -> AI stuck on duplicate. KICKING to random spot!\n",
            "   [Exp #63] Ran Fidelity 0 (Cost: 10). Result: 20.2406 g/L. Budget Left: 6460\n",
            "   -> AI stuck on duplicate. KICKING to random spot!\n",
            "   [Exp #64] Ran Fidelity 0 (Cost: 10). Result: 6.1291 g/L. Budget Left: 6450\n",
            "   -> AI stuck on duplicate. KICKING to random spot!\n",
            "   [Exp #65] Ran Fidelity 0 (Cost: 10). Result: 1.3672 g/L. Budget Left: 6440\n",
            "   -> AI stuck on duplicate. KICKING to random spot!\n",
            "   [Exp #66] Ran Fidelity 0 (Cost: 10). Result: 4.9036 g/L. Budget Left: 6430\n",
            "   -> AI stuck on duplicate. KICKING to random spot!\n",
            "   [Exp #67] Ran Fidelity 0 (Cost: 10). Result: 19.1508 g/L. Budget Left: 6420\n",
            "   -> AI stuck on duplicate. KICKING to random spot!\n",
            "   [Exp #68] Ran Fidelity 0 (Cost: 10). Result: 1.3669 g/L. Budget Left: 6410\n",
            "   -> AI stuck on duplicate. KICKING to random spot!\n",
            "   [Exp #69] Ran Fidelity 0 (Cost: 10). Result: 7.8237 g/L. Budget Left: 6400\n",
            "   -> AI stuck on duplicate. KICKING to random spot!\n",
            "   [Exp #70] Ran Fidelity 0 (Cost: 10). Result: 1.3657 g/L. Budget Left: 6390\n",
            "   [Exp #71] Ran Fidelity 0 (Cost: 10). Result: 20.1956 g/L. Budget Left: 6380\n",
            "   -> AI stuck on duplicate. KICKING to random spot!\n",
            "   [Exp #72] Ran Fidelity 0 (Cost: 10). Result: 3.2597 g/L. Budget Left: 6370\n",
            "   -> AI stuck on duplicate. KICKING to random spot!\n",
            "   [Exp #73] Ran Fidelity 0 (Cost: 10). Result: 1.3769 g/L. Budget Left: 6360\n",
            "   -> AI stuck on duplicate. KICKING to random spot!\n",
            "   [Exp #74] Ran Fidelity 0 (Cost: 10). Result: 1.3644 g/L. Budget Left: 6350\n",
            "   -> AI stuck on duplicate. KICKING to random spot!\n",
            "   [Exp #75] Ran Fidelity 0 (Cost: 10). Result: 21.7309 g/L. Budget Left: 6340\n",
            "   -> AI stuck on duplicate. KICKING to random spot!\n",
            "   [Exp #76] Ran Fidelity 0 (Cost: 10). Result: 8.9108 g/L. Budget Left: 6330\n",
            "   -> AI stuck on duplicate. KICKING to random spot!\n",
            "   [Exp #77] Ran Fidelity 0 (Cost: 10). Result: 5.4515 g/L. Budget Left: 6320\n",
            "   -> AI stuck on duplicate. KICKING to random spot!\n",
            "   [Exp #78] Ran Fidelity 0 (Cost: 10). Result: 13.2859 g/L. Budget Left: 6310\n",
            "   -> AI stuck on duplicate. KICKING to random spot!\n",
            "   [Exp #79] Ran Fidelity 0 (Cost: 10). Result: 1.3633 g/L. Budget Left: 6300\n",
            "\n",
            "!!! ENTERING FINAL VERIFICATION PHASE (Top 3 Candidates) !!!\n",
            "   > Verifying Rank #1 Candidate (Source: Exp #43)...\n",
            "   [Exp #80] Ran Fidelity 2 (Cost: 2100). Result: 131.0327 g/L. Budget Left: 4200\n",
            "   > Verifying Rank #2 Candidate (Source: Exp #45)...\n",
            "   [Exp #81] Ran Fidelity 2 (Cost: 2100). Result: 79.9886 g/L. Budget Left: 2100\n",
            "   > Verifying Rank #3 Candidate (Source: Exp #41)...\n",
            "   [Exp #82] Ran Fidelity 2 (Cost: 2100). Result: 74.5770 g/L. Budget Left: 0\n",
            "\n",
            "========================================\n",
            "       OPTIMIZATION REPORT       \n",
            "========================================\n",
            "Total Experiments:   82\n",
            " - Low Fid (0) Runs: 65\n",
            " - Mid Fid (1) Runs: 14\n",
            " - High Fid (2) Runs:3\n",
            "----------------------------------------\n",
            "Best Basic Scout (Fid 0):   21.7309 g/L\n",
            "Best Adv. Scout (Fid 1):    135.9694 g/L\n",
            "----------------------------------------\n",
            "🏆 BEST REAL RESULT (Fid 2): 131.0327 g/L\n",
            "========================================\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "test_botorch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}